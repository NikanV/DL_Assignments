{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e5fd69e",
   "metadata": {
    "id": "5e5fd69e"
   },
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"https://www.sharif.ir/documents/20124/0/logo-fa-IR.png/4d9b72bc-494b-ed5a-d3bb-e7dfd319aec8?t=1609608338755\" alt=\"Logo\" width=\"200\">\n",
    "    <p><b>HW1 @ Deep Learning Course, Dr. Soleymani</b></p>\n",
    "    <p><b>ŸêDesinged by Payam Taebi</b></p>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sJ87dPg0aerE",
   "metadata": {
    "id": "sJ87dPg0aerE"
   },
   "source": [
    "\n",
    "*Full Name:* Nikan Vasei\n",
    "\n",
    "*Student Number:* 400105303"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pIJQvTjfMMDK"
   },
   "source": [
    "\n",
    "\n",
    "# Overview(100 Points)\n",
    "\n",
    "This notebook is primarily for exploration rather than heavy implementation. We aim to examine the performance of various optimization methods on different functions. The submission for this exercise is in-person, so please experiment with the parameters in this section to better understand the challenges of each part.\n",
    "\n",
    "We will define several 1D and 2D functions (e.g., Rastrigin, Schwefel, Rosenbrock, Ackley, Beale, Eggholder, Ill-Conditioned Convex functions) using PyTorch. These functions serve as challenging test cases for our optimizers.\n",
    "\n",
    "Implement different optimization algorithms:\n",
    "We cover first-order methods (SGD, SGD with Momentum, Nesterov Accelerated Gradient, Adagrad, RMSprop, Adam, Nadam) and second-order methods (Newton's Method, L-BFGS). Each method's update rule is implemented and explained.\n",
    "\n",
    "These are some of the components we used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RmYX4Q6XKiBi"
   },
   "source": [
    "# First Section\n",
    "\n",
    "In the first section, we have a set of functions for demonstration. Do not modify these; they are implemented to display the functions in this section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Upm-lioPFXIO"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import plotly.graph_objects as go\n",
    "def plot_1d_function_torch(func, domain, global_x, title=\"1D Function Plot\", initial_x=None, path=None):\n",
    "    \"\"\"\n",
    "    Plot a 1D function (defined in PyTorch) and mark the global minimum.\n",
    "    Optionally, mark an initial point and the optimization path with directional arrows.\n",
    "\n",
    "    Parameters:\n",
    "      - func: a function that accepts a torch tensor and returns a torch tensor.\n",
    "      - domain: tuple (xmin, xmax)\n",
    "      - global_x: x-coordinate of the global minimum.\n",
    "      - title: title for the plot.\n",
    "      - initial_x: (optional) x-coordinate of the initial point.\n",
    "      - path: (optional) a list or numpy array of x-values representing the optimization path.\n",
    "    \"\"\"\n",
    "    x_np = np.linspace(domain[0], domain[1], 1000)\n",
    "    x_torch = torch.tensor(x_np, dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        y_torch = func(x_torch)\n",
    "    y_np = y_torch.numpy()\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(x_np, y_np, lw=2, label=title)\n",
    "\n",
    "    # Mark initial point if provided.\n",
    "    if initial_x is not None:\n",
    "        init_val = func(torch.tensor(initial_x, dtype=torch.float32)).item()\n",
    "        plt.scatter(initial_x, init_val, color='blue', s=80, zorder=5, label='Initial Point')\n",
    "\n",
    "    # Plot the optimization path if provided.\n",
    "    if path is not None:\n",
    "        path = np.array(path)\n",
    "        # Draw a line connecting the points.\n",
    "        y_path = [func(torch.tensor(p, dtype=torch.float32)).item() for p in path]\n",
    "        plt.plot(path, y_path, color='orange', linewidth=2, marker='o', markersize=4, label='Optimization Path')\n",
    "        # Draw arrows for each segment.\n",
    "        for i in range(len(path)-1):\n",
    "            start = path[i]\n",
    "            end = path[i+1]\n",
    "            y_start = func(torch.tensor(start, dtype=torch.float32)).item()\n",
    "            y_end = func(torch.tensor(end, dtype=torch.float32)).item()\n",
    "            plt.annotate(\"\",\n",
    "                         xy=(end, y_end),\n",
    "                         xytext=(start, y_start),\n",
    "                         arrowprops=dict(arrowstyle=\"->\", color='orange', lw=2))\n",
    "\n",
    "    # Mark the global minimum.\n",
    "    global_val = func(torch.tensor(global_x, dtype=torch.float32)).item()\n",
    "    plt.scatter(global_x, global_val, color='red', s=80, zorder=5, label='Global Minimum')\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('f(x)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "def plot_2d_contour_torch(func, x_domain, y_domain, global_point, title=\"2D Function Contour\",\n",
    "                            levels=50, cmap='viridis', initial_point=None, path=None):\n",
    "    \"\"\"\n",
    "    Plot a 2D function (defined in PyTorch) using a contour plot, marking the global minimum.\n",
    "    Optionally, mark an initial point and draw the optimization path with arrows indicating direction.\n",
    "\n",
    "    Parameters:\n",
    "      - func: a function that accepts a torch tensor of shape (2, H, W) and returns a tensor of shape (H, W).\n",
    "      - x_domain: tuple (xmin, xmax)\n",
    "      - y_domain: tuple (ymin, ymax)\n",
    "      - global_point: tuple (x*, y*) of the global minimum.\n",
    "      - title: title for the plot.\n",
    "      - levels: number of contour levels.\n",
    "      - cmap: colormap.\n",
    "      - initial_point: (optional) tuple (x0, y0) for the initial point.\n",
    "      - path: (optional) numpy array of shape (n, 2) representing the optimization path.\n",
    "    \"\"\"\n",
    "    x_np = np.linspace(x_domain[0], x_domain[1], 400)\n",
    "    y_np = np.linspace(y_domain[0], y_domain[1], 400)\n",
    "    X_np, Y_np = np.meshgrid(x_np, y_np)\n",
    "\n",
    "    # Create input grid tensor with shape (2, H, W)\n",
    "    grid = torch.tensor(np.stack([X_np, Y_np], axis=0), dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        Z_torch = func(grid)\n",
    "    Z_np = Z_torch.numpy()\n",
    "    vmin, vmax = Z_np.min(), Z_np.max()\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    cp = plt.contourf(X_np, Y_np, Z_np, levels=levels, cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "    plt.colorbar(cp)\n",
    "    plt.contour(X_np, Y_np, Z_np, levels=15, colors='black', alpha=0.5)\n",
    "\n",
    "    # Mark initial point if provided.\n",
    "    if initial_point is not None:\n",
    "        plt.scatter(initial_point[0], initial_point[1], color='blue', s=100, marker='o', label='Initial Point')\n",
    "\n",
    "    # Plot the optimization path if provided.\n",
    "    if path is not None:\n",
    "        path = np.array(path)\n",
    "        plt.plot(path[:, 0], path[:, 1], marker='o', color='orange', markersize=4, linewidth=2, label='Optimization Path')\n",
    "        for i in range(len(path) - 1):\n",
    "            start = path[i]\n",
    "            end = path[i+1]\n",
    "            plt.annotate(\"\",\n",
    "                         xy=(end[0], end[1]),\n",
    "                         xytext=(start[0], start[1]),\n",
    "                         arrowprops=dict(arrowstyle=\"->\", color='orange', lw=2))\n",
    "\n",
    "    # Mark the global minimum.\n",
    "    plt.scatter(global_point[0], global_point[1], color='red', s=100, marker='*', label='Global Minimum')\n",
    "\n",
    "    plt.title(title + \" (Contour Plot)\")\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "def plot_interactive_3d_torch(func, x_domain, y_domain, global_point, title=\"Interactive 3D Plot\",\n",
    "                              colorscale='viridis', initial_point=None, path=None):\n",
    "    \"\"\"\n",
    "    Create an interactive 3D surface plot using Plotly for a 2D function defined in PyTorch.\n",
    "    Assumes the input is a tensor with two channels: first channel for x and second for y.\n",
    "    Optionally, marks an initial point and draws cones (arrows) to indicate direction along the optimization path.\n",
    "\n",
    "    Parameters:\n",
    "      - func: a function that accepts a torch tensor of shape (2, H, W) and returns a tensor of shape (H, W).\n",
    "      - x_domain: tuple (xmin, xmax)\n",
    "      - y_domain: tuple (ymin, ymax)\n",
    "      - global_point: tuple (x*, y*) for the global minimum.\n",
    "      - title: title for the plot.\n",
    "      - colorscale: Plotly colorscale.\n",
    "      - initial_point: (optional) tuple (x0, y0) for the initial point.\n",
    "      - path: (optional) numpy array of shape (n, 2) representing the optimization path.\n",
    "    \"\"\"\n",
    "    # Create grid for the surface.\n",
    "    x_np = np.linspace(x_domain[0], x_domain[1], 200)\n",
    "    y_np = np.linspace(y_domain[0], y_domain[1], 200)\n",
    "    X_np, Y_np = np.meshgrid(x_np, y_np)\n",
    "\n",
    "    # Create grid tensor with shape (2, H, W)\n",
    "    grid = torch.tensor(np.stack([X_np, Y_np], axis=0), dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        Z_torch = func(grid)\n",
    "    Z_np = Z_torch.numpy()\n",
    "\n",
    "    surface = go.Surface(x=X_np, y=Y_np, z=Z_np, colorscale=colorscale, opacity=0.9, name=title)\n",
    "    data = [surface]\n",
    "\n",
    "    # Optionally add initial point.\n",
    "    if initial_point is not None:\n",
    "        init_val = func(torch.tensor([initial_point[0], initial_point[1]], dtype=torch.float32)).item()\n",
    "        scatter_initial = go.Scatter3d(\n",
    "            x=[initial_point[0]],\n",
    "            y=[initial_point[1]],\n",
    "            z=[init_val],\n",
    "            mode='markers',\n",
    "            marker=dict(size=6, color='blue'),\n",
    "            name='Initial Point'\n",
    "        )\n",
    "        data.append(scatter_initial)\n",
    "\n",
    "    # Optionally add optimization path.\n",
    "    if path is not None:\n",
    "        path = np.array(path)\n",
    "        z_path = []\n",
    "        for pt in path:\n",
    "            z_val = func(torch.tensor([pt[0], pt[1]], dtype=torch.float32)).item()\n",
    "            z_path.append(z_val)\n",
    "        scatter_path = go.Scatter3d(\n",
    "            x=path[:, 0],\n",
    "            y=path[:, 1],\n",
    "            z=z_path,\n",
    "            mode='lines+markers',\n",
    "            line=dict(color='orange', width=4),\n",
    "            marker=dict(size=4, color='orange'),\n",
    "            name='Optimization Path'\n",
    "        )\n",
    "        data.append(scatter_path)\n",
    "\n",
    "        # Add cone traces for directional arrows.\n",
    "        cone_x = []\n",
    "        cone_y = []\n",
    "        cone_z = []\n",
    "        cone_u = []\n",
    "        cone_v = []\n",
    "        cone_w = []\n",
    "        for i in range(len(path) - 1):\n",
    "            start = path[i]\n",
    "            end = path[i+1]\n",
    "            cone_x.append(start[0])\n",
    "            cone_y.append(start[1])\n",
    "            # Evaluate z at start.\n",
    "            z_start = func(torch.tensor([start[0], start[1]], dtype=torch.float32)).item()\n",
    "            cone_z.append(z_start)\n",
    "            cone_u.append(end[0] - start[0])\n",
    "            cone_v.append(end[1] - start[1])\n",
    "            z_end = func(torch.tensor([end[0], end[1]], dtype=torch.float32)).item()\n",
    "            cone_w.append(z_end - z_start)\n",
    "        if len(cone_x) > 0:\n",
    "            cone_trace = go.Cone(\n",
    "                x=cone_x,\n",
    "                y=cone_y,\n",
    "                z=cone_z,\n",
    "                u=cone_u,\n",
    "                v=cone_v,\n",
    "                w=cone_w,\n",
    "                sizemode=\"absolute\",\n",
    "                sizeref=0.5,\n",
    "                anchor=\"tail\",\n",
    "                showscale=False,\n",
    "                colorscale=[[0, 'orange'], [1, 'orange']],\n",
    "                name=\"Step Arrows\"\n",
    "            )\n",
    "            data.append(cone_trace)\n",
    "\n",
    "    # Add global minimum.\n",
    "    global_tensor = torch.tensor([global_point[0], global_point[1]], dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        global_z = func(global_tensor).item()\n",
    "    scatter_global = go.Scatter3d(\n",
    "        x=[global_point[0]],\n",
    "        y=[global_point[1]],\n",
    "        z=[global_z],\n",
    "        mode='markers',\n",
    "        marker=dict(size=8, color='red', symbol='diamond'),\n",
    "        name='Global Minimum'\n",
    "    )\n",
    "    data.append(scatter_global)\n",
    "\n",
    "    fig = go.Figure(data=data)\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        scene=dict(\n",
    "            xaxis_title='x',\n",
    "            yaxis_title='y',\n",
    "            zaxis_title='f(x,y)'\n",
    "        ),\n",
    "        autosize=True\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def plot_loss_vs_epoch(loss_func, path):\n",
    "\n",
    "    \"\"\"\n",
    "    Given a loss function and an optimization path, compute the loss at each epoch and plot it.\n",
    "\n",
    "    Parameters:\n",
    "      - loss_func: A function that accepts a torch tensor representing parameters and returns a scalar loss.\n",
    "      - path: A list or NumPy array of parameter values (e.g. from the optimizer).\n",
    "              Each element should be convertible to a torch tensor.\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    # Loop over each parameter value in the path.\n",
    "    for p in path:\n",
    "        # Convert parameter p to a torch tensor (assume float32).\n",
    "        # p can be a 1D array or a multi-dimensional array.\n",
    "        p_tensor = torch.tensor(p, dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            loss_val = loss_func(p_tensor).item()\n",
    "        losses.append(loss_val)\n",
    "\n",
    "    epochs = np.arange(len(losses))\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(epochs, losses, marker='o', color='blue', linewidth=2)\n",
    "    plt.title(\"Loss vs. Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pAjw8p8FKqcr"
   },
   "source": [
    "# Next Section (18 Points)\n",
    "\n",
    "In the following section, we examine all the functions, each of which presents its own unique challenge. These functions are both one-dimensional and two-dimensional.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "smXMurxMlYMW"
   },
   "source": [
    "# 1D Rastrigin Function\n",
    "\n",
    "**Mathematical Definition:**\n",
    "\n",
    "$$\n",
    "f(x) = x^2 - 10 \\cos\\left(2\\pi x\\right) + 10\n",
    "$$\n",
    "\n",
    "**Domain:**  \n",
    "\\\\( x \\in [-5,\\, 5] \\\\)\n",
    "\n",
    "**Global Minimum:**  \n",
    "\\\\( x = 0 \\\\) with \\\\( f(0)=0 \\\\)\n",
    "\n",
    "**Explanation:**  \n",
    "This function is **non-convex** and exhibits many local minima due to the cosine modulation. Its oscillatory behavior over \\\\( [-5,5] \\\\) makes it a popular test for global optimization methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aj3JwNZya2rZ"
   },
   "outputs": [],
   "source": [
    "# 1D Rastrigin function in PyTorch.\n",
    "def rastrigin_1d_torch(x: torch.Tensor) -> torch.Tensor:\n",
    "    # TODO: Implement the 1D Rastrigin function using PyTorch operations\n",
    "    return x**2 - 10 * torch.cos(2 * torch.pi * x) + 10\n",
    "\n",
    "# Plot the 1D Rastrigin function.\n",
    "plot_1d_function_torch(rastrigin_1d_torch, [-5, 5], global_x=0, title=\"1D Rastrigin Function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_R4yW7ml0KQ"
   },
   "source": [
    "# 1D Schwefel Function\n",
    "\n",
    "**Mathematical Definition:**\n",
    "\n",
    "$$\n",
    "f(x) = 418.9829 - x \\sin\\Bigl(\\sqrt{|x|}\\Bigr)\n",
    "$$\n",
    "\n",
    "**Domain:**  \n",
    "\\\\( x \\in [0,\\, 500] \\\\)\n",
    "\n",
    "**Global Minimum:**  \n",
    "Approximately at \\\\( x \\approx 420.9687 \\\\)\n",
    "\n",
    "**Explanation:**  \n",
    "This function is highly **non-convex** and rugged with many deceptive local minima. Its large domain accentuates the difficulty in escaping local traps, making it a challenging benchmark for optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ITGCVgusa2tS"
   },
   "outputs": [],
   "source": [
    "# 1D Schwefel function in PyTorch.\n",
    "def schwefel_1d_torch(x: torch.Tensor) -> torch.Tensor:\n",
    "    # TODO: Implement the 1D Schwefel function using PyTorch operations\n",
    "    return 418.9829 - x * torch.sin(torch.sqrt(torch.abs(x)))\n",
    "\n",
    "# Plot the 1D Schwefel function.\n",
    "plot_1d_function_torch(schwefel_1d_torch, [0, 500], global_x=420.9687, title=\"1D Schwefel Function\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2qGbJ8Nl42Z"
   },
   "source": [
    "# 1D Ill-Conditioned Convex Function\n",
    "\n",
    "**Mathematical Definition:**\n",
    "\n",
    "$$\n",
    "f(x) = \\ln\\Bigl(1 + e^{10x}\\Bigr) + x^2\n",
    "$$\n",
    "\n",
    "**Domain:**  \n",
    "\\\\( x \\in [-5,\\, 5] \\\\)\n",
    "\n",
    "**Global Minimum:**  \n",
    "Approximately at \\\\( x \\approx -0.3 \\\\)\n",
    "\n",
    "**Explanation:**  \n",
    "This function is **convex** (and therefore has a unique global minimum), but it is _ill-conditioned_ because the steep exponential part for \\\\( x > 0 \\\\) creates a rapidly changing gradient. It tests how well an optimizer can adjust its step size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w1NQwY4sa2vF"
   },
   "outputs": [],
   "source": [
    "# 1D Ill-Conditioned Convex function in PyTorch.\n",
    "def ill_conditioned_convex_1d_torch(x: torch.Tensor) -> torch.Tensor:\n",
    "    # TODO: Implement the 1D ill-conditioned convex function using PyTorch operations\n",
    "    return torch.log(1 + torch.exp(10 * x)) + x**2\n",
    "\n",
    "# Plot the 1D Ill-Conditioned Convex function.\n",
    "plot_1d_function_torch(ill_conditioned_convex_1d_torch, [-5, 5], global_x=-0.3, title=\"1D Ill-Conditioned Convex Function\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cs4YCMZFmB9v"
   },
   "source": [
    "# Rosenbrock Function (2D)\n",
    "\n",
    "**Mathematical Definition:**\n",
    "\n",
    "$$\n",
    "f(x, y) = (1 - x)^2 + 100 \\,(y - x^2)^2\n",
    "$$\n",
    "\n",
    "**Domain:**  \n",
    "\\\\( x \\in [-2,\\, 2] \\\\) and \\\\( y \\in [-1,\\, 3] \\\\)\n",
    "\n",
    "**Global Minimum:**  \n",
    "At \\\\( (x,y) = (1,1) \\\\) with \\\\( f(1,1)=0 \\\\)\n",
    "\n",
    "**Explanation:**  \n",
    "The Rosenbrock function is **non-convex** despite its smoothness. Its narrow, curved valley makes it challenging for gradient-based optimizers to converge to the global minimum. This function is a classical benchmark for testing optimization algorithms.\n",
    "\n",
    "In the next cells, we plot it both as a static contour and as an interactive 3D plot. We'll also mark an initial point (for example, \\\\( (-1,1.5) \\\\)) and the optimization path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PzqERVuBmBI5"
   },
   "outputs": [],
   "source": [
    "# Define the Rosenbrock function for 2D inputs in PyTorch.\n",
    "# The input is a tensor of shape (2, H, W) where the first channel is x and the second is y.\n",
    "def rosenbrock_2d_torch(X: torch.Tensor) -> torch.Tensor:\n",
    "    # TODO: Implement the 2D Rosenbrock function using PyTorch operations\n",
    "    return (1 - X[0]) ** 2 + 100 * (X[1] - X[0]**2) ** 2\n",
    "\n",
    "# Plot using static contour.\n",
    "plot_2d_contour_torch(rosenbrock_2d_torch, [-2, 2], [-1, 3], global_point=(1, 1),\n",
    "                      title=\"Rosenbrock Function (2D)\")\n",
    "\n",
    "# Plot using interactive 3D.\n",
    "plot_interactive_3d_torch(rosenbrock_2d_torch, [-2, 2], [-1, 3], global_point=(1, 1),\n",
    "                          title=\"Interactive 3D Rosenbrock Function\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wf-LTGmhYtYI"
   },
   "source": [
    "# 2D Rastrigin Function\n",
    "\n",
    "**Mathematical Definition:**\n",
    "\n",
    "$$\n",
    "f(x, y) = x^2 + y^2 - 10 \\Bigl[\\cos\\left(2\\pi x\\right) + \\cos\\left(2\\pi y\\right)\\Bigr] + 20\n",
    "$$\n",
    "\n",
    "**Domain:**  \n",
    "\\\\( x, y \\in [-5,\\, 5] \\\\)\n",
    "\n",
    "**Global Minimum:**  \n",
    "At \\\\( (0,0) \\\\) with \\\\( f(0,0)=0 \\\\)\n",
    "\n",
    "**Explanation:**  \n",
    "The 2D Rastrigin function is highly **non-convex** and multimodal, featuring many local minima. Its oscillatory behavior in both dimensions makes it a demanding test for optimization techniques.\n",
    "\n",
    "Below, we display both the static contour and interactive 3D plots with an example initial point (e.g., \\\\( (3,3) \\\\)) and a dummy optimization path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZTCSiYLqYvFR"
   },
   "outputs": [],
   "source": [
    "# Define the 2D Rastrigin function in PyTorch.\n",
    "def rastrigin_2d_torch(X: torch.Tensor) -> torch.Tensor:\n",
    "    # TODO: Implement the 2D Rastrigin function using PyTorch operations\n",
    "    return X[0]**2 + X[1]**2 - 10 * (torch.cos(2 * torch.pi * X[0]) + torch.cos(2 * torch.pi * X[1])) + 20\n",
    "\n",
    "# Plot as a static contour.\n",
    "plot_2d_contour_torch(rastrigin_2d_torch, [-5, 5], [-5, 5], global_point=(0, 0),\n",
    "                      title=\"2D Rastrigin Function\", cmap='plasma')\n",
    "\n",
    "# Plot as an interactive 3D surface.\n",
    "plot_interactive_3d_torch(rastrigin_2d_torch, [-5, 5], [-5, 5], global_point=(0, 0),\n",
    "                          title=\"Interactive 3D Rastrigin Function\", colorscale='plasma')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQfFakkSYtap"
   },
   "source": [
    "# Ackley Function (2D)\n",
    "\n",
    "**Mathematical Definition:**\n",
    "\n",
    "$$\n",
    "f(x, y) = -20 \\exp\\!\\left(-0.2 \\sqrt{\\frac{x^2+y^2}{2}}\\right) - \\exp\\!\\left(\\frac{\\cos\\left(2\\pi x\\right)+\\cos\\left(2\\pi y\\right)}{2}\\right) + 20 + e\n",
    "$$\n",
    "\n",
    "**Domain:**  \n",
    "\\\\( x, y \\in [-5,\\, 5] \\\\)\n",
    "\n",
    "**Global Minimum:**  \n",
    "At \\\\( (0,0) \\\\) with \\\\( f(0,0)=0 \\\\)\n",
    "\n",
    "**Explanation:**  \n",
    "The Ackley function features a nearly flat outer region with a deep, narrow central pit, creating a challenging landscape for optimization algorithms. This function is popular for testing global optimization techniques.\n",
    "\n",
    "We now show both the contour and interactive 3D plots with an initial point (e.g., \\\\( (3,3) \\\\)) and an optimization path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XJSmNBCZY55h"
   },
   "outputs": [],
   "source": [
    "# Define the 2D Ackley function in PyTorch.\n",
    "def ackley_2d_torch(X: torch.Tensor, a=20, b=0.2, c=2*np.pi) -> torch.Tensor:\n",
    "    # TODO: Implement the 2D Ackley function using PyTorch operations\n",
    "    return -20 * torch.exp(-0.2 * torch.sqrt((X[0]**2 + X[1]**2) / 2)) - torch.exp((torch.cos(2 * torch.pi * X[0]) + torch.cos(2 * torch.pi * X[1])) / 2) + 20 + torch.e\n",
    "\n",
    "# Plot as a static contour.\n",
    "plot_2d_contour_torch(ackley_2d_torch, [-5, 5], [-5, 5], global_point=(0, 0),\n",
    "                      title=\"Ackley Function (2D)\", cmap='inferno')\n",
    "\n",
    "# Plot as an interactive 3D surface.\n",
    "plot_interactive_3d_torch(ackley_2d_torch, [-5, 5], [-5, 5], global_point=(0, 0),\n",
    "                          title=\"Interactive 3D Ackley Function\", colorscale='inferno')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Z9KNdHsYtfH"
   },
   "source": [
    "# Beale Function (2D)\n",
    "\n",
    "**Mathematical Definition:**\n",
    "\n",
    "$$\n",
    "f(x, y) = \\left(1.5 - x + xy\\right)^2 + \\left(2.25 - x + xy^2\\right)^2 + \\left(2.625 - x + xy^3\\right)^2\n",
    "$$\n",
    "\n",
    "**Domain:**  \n",
    "\\\\( x, y \\in [-4.5,\\, 4.5] \\\\)\n",
    "\n",
    "**Global Minimum:**  \n",
    "At \\\\( (3, 0.5) \\\\) with \\\\( f(3,0.5)=0 \\\\)\n",
    "\n",
    "**Explanation:**  \n",
    "The Beale function is **non-convex** with several local minima, making it a classical benchmark for global optimization. Its complex structure challenges optimizers in 2D space.\n",
    "\n",
    "Below, we provide both a static contour and an interactive 3D plot with an initial point (e.g., \\\\( (0,0) \\\\)) and an optimization path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LgxBtD0XZCsJ"
   },
   "outputs": [],
   "source": [
    "# Define the 2D Beale function in PyTorch.\n",
    "def beale_2d_torch(X: torch.Tensor) -> torch.Tensor:\n",
    "    # TODO: Implement the 2D Beale function using PyTorch operations\n",
    "    return (1.5 - X[0] + X[0] * X[1]) ** 2 + (2.25 - X[0] + X[0] * X[1]) ** 2 + (2.625 - X[0] + X[0] * X[1] ** 3) ** 2\n",
    "\n",
    "# Plot as a static contour.\n",
    "plot_2d_contour_torch(beale_2d_torch, [-4.5, 4.5], [-4.5, 4.5], global_point=(3, 0.5),\n",
    "                      title=\"Beale Function (2D)\", cmap='Spectral')\n",
    "\n",
    "# Plot as an interactive 3D surface.\n",
    "plot_interactive_3d_torch(beale_2d_torch, [-4.5, 4.5], [-4.5, 4.5], global_point=(3, 0.5),\n",
    "                          title=\"Interactive 3D Beale Function\", colorscale='Spectral')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uI84q3nqYthc"
   },
   "source": [
    "# Eggholder Function (2D)\n",
    "\n",
    "**Mathematical Definition:**\n",
    "\n",
    "$$\n",
    "f(x, y) = - (y+47) \\sin\\!\\left(\\sqrt{\\left|\\frac{x}{2} + (y+47)\\right|}\\right) - x \\sin\\!\\left(\\sqrt{\\left|x - (y+47)\\right|}\\right)\n",
    "$$\n",
    "\n",
    "**Domain:**  \n",
    "\\\\( x, y \\in [-512,\\, 512] \\\\)\n",
    "\n",
    "**Global Minimum:**  \n",
    "Approximately at \\\\( (512,\\, 404.2319) \\\\)\n",
    "\n",
    "**Explanation:**  \n",
    "The Eggholder function is extremely **non-convex** with a rugged, highly oscillatory landscape that contains many local minima. It is one of the most challenging test functions in optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u1cvpJUtZTTj"
   },
   "outputs": [],
   "source": [
    "# Define the 2D Eggholder function in PyTorch.\n",
    "def eggholder_2d_torch(X: torch.Tensor) -> torch.Tensor:\n",
    "    # TODO: Implement the 2D Eggholder function using PyTorch operations\n",
    "    return -(X[1] + 47) * torch.sin(torch.sqrt(torch.abs(X[0] / 2 + (X[1] + 47)))) - X[0] * torch.sin(torch.sqrt(torch.abs(X[0] - (X[1] + 47))))\n",
    "\n",
    "# Plot as a static contour.\n",
    "plot_2d_contour_torch(eggholder_2d_torch, [-512, 512], [-512, 512], global_point=(512, 404.2319),\n",
    "                      title=\"Eggholder Function (2D)\", cmap='inferno')\n",
    "\n",
    "# Plot as an interactive 3D surface.\n",
    "plot_interactive_3d_torch(eggholder_2d_torch, [-512, 512], [-512, 512], global_point=(512, 404.2319),\n",
    "                          title=\"Interactive 3D Eggholder Function\", colorscale='inferno')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_jSswuwYtkH"
   },
   "source": [
    "# Ill-Conditioned Convex Function (2D)\n",
    "\n",
    "**Mathematical Definition:**\n",
    "\n",
    "$$\n",
    "f(x, y) = 100 \\Bigl(x\\cos\\theta - y\\sin\\theta\\Bigr)^2 + \\Bigl(x\\sin\\theta + y\\cos\\theta\\Bigr)^2 \\quad \\text{with } \\theta = \\frac{\\pi}{6}\n",
    "$$\n",
    "\n",
    "**Domain:**  \n",
    "\\\\( x, y \\in [-5,\\, 5] \\\\)\n",
    "\n",
    "**Global Minimum:**  \n",
    "At \\\\( (0,0) \\\\) with \\\\( f(0,0)=0 \\\\)\n",
    "\n",
    "**Explanation:**  \n",
    "This function is **convex** (thus it has a unique global minimum) but is _ill-conditioned_ due to the rotation and different scaling along the rotated axes. Its behavior tests an optimizer‚Äôs ability to deal with steep versus flat directions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qGdz98dfZhVC"
   },
   "outputs": [],
   "source": [
    "# Define the 2D Ill-Conditioned Convex function in PyTorch.\n",
    "def ill_conditioned_convex_2d_torch(X: torch.Tensor, theta=torch.tensor(np.pi/6)) -> torch.Tensor:\n",
    "    # TODO: Implement the 2D ill-conditioned convex function using PyTorch operations\n",
    "    return 100 * (X[0] * torch.cos(theta) - X[1] * torch.sin(theta)) ** 2 + (X[0] * torch.sin(theta) - X[1] * torch.cos(theta)) ** 2\n",
    "\n",
    "# Plot as a static contour.\n",
    "plot_2d_contour_torch(ill_conditioned_convex_2d_torch, [-5, 5], [-5, 5], global_point=(0, 0),\n",
    "                      title=\"Ill-Conditioned Convex Function (2D)\", cmap='viridis')\n",
    "\n",
    "# Plot as an interactive 3D surface.\n",
    "plot_interactive_3d_torch(ill_conditioned_convex_2d_torch, [-5, 5], [-5, 5], global_point=(0, 0),\n",
    "                          title=\"Interactive 3D Ill-Conditioned Convex Function (2D)\", colorscale='viridis')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZoK5omVIK8Ru"
   },
   "source": [
    "# Question (12 Points)\n",
    "\n",
    "For each of the functions presented above, what unique challenges do you think they offer? Consider aspects such as steep gradients, saddle points, local minima, flat regions, and other characteristics that could affect the optimization process.\n",
    "\n",
    "Please provide a brief explanation for each function, describing the potential difficulties in learning or optimizing it.\n",
    "\n",
    "\n",
    "## Answer\n",
    "\n",
    "1. ### 1D Rastrigin Function\n",
    "   - Non-convex surface\n",
    "   - Multiple local minima due to the oscillatory cosine term (Traps gradient-based methods away from the global optimum)\n",
    "   - Steep valleys and flat regions (Makes step-size selection tricky)\n",
    "   - Sensitivity to initialization (Where you start heavily influences which minima you find)\n",
    "\n",
    "1. ### 1D Schwefel Function\n",
    "   - Non-convex surface\n",
    "   - Numerous local minima (‚Äútrap‚Äù points along the domain)\n",
    "   - Highly rugged landscape (Rapid fluctuations in the function‚Äôs slope can disrupt smooth convergence)\n",
    "   - Large domain (A wide search interval increases the chance of getting stuck far from the global minimum)\n",
    "   - Steep slopes near certain regions (Can cause overshooting or unstable steps)\n",
    "\n",
    "1. ### 1D Ill-Conditioned Convex Function\n",
    "   - Ill-conditioning (Leads to slow or unstable convergence)\n",
    "   - Rapidly changing gradient (Can produce large updates in certain regions)\n",
    "   - Potential numerical stability issues\n",
    "\n",
    "1. ### 2D Rosenbrock Function\n",
    "   - Non-convex surface\n",
    "   - Long, narrow valley (Requires precise steps to avoid bouncing or stalling)\n",
    "   - Slow convergence near the valley\n",
    "   - Sensitivity to hyperparameters\n",
    "\n",
    "1. ### 2D Rastrigin Function\n",
    "   - Non-convex surface\n",
    "   - Multidimensional oscillations (The function repeats peaks and valleys along both x and y)\n",
    "   - Abundance of local minima\n",
    "   - Steep drop-offs and gentle plateaus\n",
    "\n",
    "1. ### 2D Ackley Function\n",
    "   - Non-convex with oscillatory components\n",
    "   - Wide flat region (Far from the origin, gradients are tiny and slow to guide the optimizer)\n",
    "   - Sharp basin around the global minimum (Close to zero, the function drops quickly and can overshoot)\n",
    "   - Sensitivity to initialization\n",
    "\n",
    "1. ### 2D Beale Function\n",
    "   - Non-convex surface\n",
    "   - Multiple local minima\n",
    "   - Complex interaction of terms\n",
    "   - Steep vs. flat regions\n",
    "\n",
    "1. ### 2D Eggholder Function\n",
    "   - Non-convex surface\n",
    "   - Highly oscillatory and multi-modal (Numerous peaks and dips produce many local optima)\n",
    "   - Large domain\n",
    "   - Complex gradient behavior\n",
    "\n",
    "1. ### 2D Ill-Conditioned Convex Function\n",
    "   - Convex yet elongated ‚Äúvalley‚Äù (Only one global minimum, but reaching it requires careful maneuvering)\n",
    "   - Severe ill-conditioning\n",
    "   - Sensitivity to step-size/learning-rate tuning\n",
    "\n",
    "\n",
    "\n",
    "Overall we can conclude that optimizers encounter these sets of challenges in different scenarios (in general):\n",
    "\n",
    "- **Non-convexity and Multiple Local Minima** (e.g., Rastrigin, Schwefel, Ackley, Eggholder) show how easily an algorithm can be trapped away from the global optimum.\n",
    "- **Ill-Conditioning** (e.g., the Ill-Conditioned Convex functions) tests the optimizer‚Äôs ability to handle skewed or elongated ‚Äúvalleys‚Äù where gradients may vary dramatically in different directions.\n",
    "- **Rugged Landscapes and Oscillatory Behavior** (e.g., Eggholder, Rastrigin, Schwefel) underscore the importance of careful step-size/learning-rate strategies to avoid hopping between‚Äîor getting stuck in‚Äîlocal basins.\n",
    "- **Smooth Yet Tricky Surfaces** (e.g., Rosenbrock, Beale) can still pose considerable difficulty when the gradient is very small near narrow valleys or curved ravines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u5LWqXSXYtmc"
   },
   "source": [
    "# Implementing an Optimization Method (5 Points)\n",
    "\n",
    "Now that we've discussed the general structure of our optimization function, let's explore how to implement a specific optimization method. In this section, we'll walk through the steps required to define an update rule for an optimizer. This involves:\n",
    "\n",
    "1. **Computing the Loss and its Derivatives:**  \n",
    "   We evaluate the loss function for the current parameters, compute the gradient, and optionally calculate the Hessian if using a second-order method.\n",
    "\n",
    "2. **Applying the Update Rule:**  \n",
    "   Based on the computed derivatives, we update the parameters using the specific logic of our chosen optimization method (e.g., adjusting the parameters using SGD, Adam, or Newton's method).\n",
    "\n",
    "3. **Maintaining State:**  \n",
    "   For methods that require momentum or other historical data, we update and maintain a state dictionary that carries this information from one iteration to the next.\n",
    "\n",
    "By following these steps, we can iteratively update the parameters to minimize the loss function and analyze the optimization trajectory. Let's move on to see this process in action.\n",
    "\n",
    "# General Optimization Function Explanation\n",
    "\n",
    "The `optimize` function provides a unified framework for iterative optimization methods. Whether using first-order (gradient-based) or second-order (Hessian-based) methods, the overall process remains similar: we start with an initial guess for the parameters and then iteratively update these parameters in order to minimize a given loss function.\n",
    "\n",
    "## How It Works\n",
    "\n",
    "1. **Initialization:**\n",
    "   - **Parameters:**  \n",
    "     The function receives an initial parameter tensor (`initial_params`). This tensor is cloned and set to require gradients so that we can compute derivatives with respect to it.\n",
    "   - **State Storage:**  \n",
    "     An empty dictionary `state` is initialized to store any additional information required by the update method (e.g., momentum for Adam, or curvature approximations for second-order methods).\n",
    "   - **Path Recording:**  \n",
    "     The initial parameters are stored in a list called `path`. This list will hold the parameter values after each iteration, allowing us to visualize the optimization trajectory later.\n",
    "\n",
    "2. **Iterative Update Process:**\n",
    "   - For each epoch (i.e., optimization step), the following steps occur:\n",
    "     1. **Loss Evaluation:**  \n",
    "        The loss function `loss_func` is evaluated at the current parameter values to determine how well the current parameters perform.\n",
    "     2. **Gradient Computation:**  \n",
    "        The gradient (first derivative) of the loss with respect to the parameters is computed using `loss.backward()`.  \n",
    "        - For **first-order methods**, only the gradient is used.\n",
    "        - For **second-order methods**, additional curvature information is needed.\n",
    "     3. **Hessian Computation (Optional):**  \n",
    "        If the `second_order` flag is set to `True`, the Hessian (matrix of second derivatives) is computed using PyTorch's `torch.autograd.functional.hessian`.  \n",
    "        This Hessian provides insight into the curvature of the loss surface and is useful for methods like Newton's method.\n",
    "     4. **Parameter Update:**  \n",
    "        The update function `update_func` is then called:\n",
    "        - For **first-order methods** (e.g., SGD, Adam):  \n",
    "          It is called with the current parameters, their gradient, the state, and hyperparameters.\n",
    "        - For **second-order methods** (e.g., Newton‚Äôs Method, L-BFGS):  \n",
    "          The computed Hessian is also passed.\n",
    "        This function returns updated parameter values and an updated state.\n",
    "     5. **Re-enable Gradient Tracking:**  \n",
    "        After the update, gradients are cleared and re-enabled for the next iteration.\n",
    "     6. **Path Recording:**  \n",
    "        The new parameters are added to the `path` list.\n",
    "\n",
    "3. **Return Value:**\n",
    "   - After completing all epochs, the function returns the `path` ‚Äî a list of parameter values at each iteration. This path can later be used to visualize the optimization trajectory (for example, by plotting the loss versus epochs or overlaying the path on the loss surface).\n",
    "\n",
    "## Common Underlying Principles\n",
    "\n",
    "- **Unified Structure:**  \n",
    "  All optimization methods follow a similar iterative procedure: compute the loss, compute the gradient (and optionally the Hessian), update the parameters, and record the progress.\n",
    "  \n",
    "- **First-Order vs. Second-Order Methods:**  \n",
    "  - *First-Order Methods* use only the gradient information. Their update rule typically has the form:  \n",
    "  $$\n",
    "    w_{t+1} = w_t - \\eta \\nabla f(w_t)\n",
    "  $$\n",
    "  - *Second-Order Methods* also incorporate curvature information (via the Hessian) to adjust the step direction and size:  \n",
    "    $$ w_{t+1} = w_t - \\left( H(w_t) + \\epsilon I \\right)^{-1} \\nabla f(w_t) $$\n",
    "  The `second_order` flag in our function determines whether the Hessian is computed and passed to the update function.\n",
    "\n",
    "- **State Management:**  \n",
    "  Many advanced optimizers (like Adam, RMSprop, or L-BFGS) maintain a state that includes historical information (e.g., moving averages of gradients, momentum, or curvature approximations). This state is updated at each iteration and passed to the update function to help guide the parameter updates.\n",
    "\n",
    "In summary, the `optimize` function abstracts the common iterative process of optimization, allowing us to plug in different update functions (each corresponding to a different optimization method) and compare their performance on the same problem. This modular approach makes it straightforward to experiment with a wide range of optimization algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q6FpMyx8iRWt"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def optimize(loss_func, update_func, initial_params, epochs, hyperparams, second_order=False):\n",
    "    \"\"\"\n",
    "    General optimization function supporting both first-order (gradient-based)\n",
    "    and second-order (Hessian-based) optimization methods.\n",
    "\n",
    "    Parameters:\n",
    "        - loss_func: Function to minimize. Accepts parameters and returns loss.\n",
    "        - update_func: Function that updates parameters.\n",
    "                       Signature (first-order): (params, grad, state, hyperparams) -> (new_params, new_state)\n",
    "                       Signature (second-order): (params, grad, state, hyperparams, hessian) -> (new_params, new_state)\n",
    "        - initial_params: Torch tensor representing the initial parameter values.\n",
    "        - epochs: Number of optimization steps.\n",
    "        - hyperparams: Dictionary of hyperparameters.\n",
    "        - second_order: Boolean flag indicating if second-order derivatives (Hessian) are needed.\n",
    "\n",
    "    Returns:\n",
    "        - path: List of parameter values at each step.\n",
    "    \"\"\"\n",
    "    params = initial_params.clone().detach().requires_grad_(True)\n",
    "    state = {}  # Store state (e.g., momentum, Hessian approximations, etc.)\n",
    "    path = [params.clone().detach().numpy()]  # Store initial point\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        loss = loss_func(params)  # Compute loss\n",
    "        loss.backward()  # Compute gradient\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient tracking during updates\n",
    "            grad = params.grad.clone()\n",
    "            params.grad.zero_()\n",
    "            \n",
    "            if second_order:\n",
    "                # TODO: Compute the Hessian matrix using torch.autograd.functional.hessian\n",
    "                hessian = torch.autograd.functional.hessian(loss_func, params)\n",
    "\n",
    "                # TODO: Update parameters using the update_func with second-order information\n",
    "                params, state = update_func(params, grad, state, hyperparams, hessian)\n",
    "            else:\n",
    "                # TODO: Update parameters using the update_func with first-order information\n",
    "                params, state = update_func(params, grad, state, hyperparams)\n",
    "\n",
    "            params.requires_grad_(True)  # Re-enable gradient computation\n",
    "\n",
    "        path.append(params.clone().detach().numpy())  # Store new point\n",
    "\n",
    "    return path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rIN2LRKlM3IK"
   },
   "source": [
    "# Optimization Demonstration Functions\n",
    "\n",
    "Next, we have a set of functions that display the optimization process by running it on all the functions. You may edit them if needed for better visualization, but preferably, do not modify these two sections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-PDjwmdrmYFm"
   },
   "outputs": [],
   "source": [
    "def optimize_and_plot(loss_func, update_func, initial_params, epochs, hyperparams, x_domain, y_domain=None, title=\"Optimization\" ,second_order=False):\n",
    "    \"\"\"\n",
    "    Perform optimization and automatically visualize the results for 1D or 2D functions.\n",
    "\n",
    "    Parameters:\n",
    "      - loss_func: Function to minimize. It accepts parameters and returns a scalar loss.\n",
    "      - update_func: Function that updates parameters. Signature: (params, grad, state, hyperparams) -> (new_params, new_state)\n",
    "      - initial_params: Torch tensor representing the initial parameter values.\n",
    "      - epochs: Number of optimization steps.\n",
    "      - hyperparams: Dictionary of hyperparameters for the update function.\n",
    "      - x_domain: Tuple (xmin, xmax) defining the domain for plotting.\n",
    "      - y_domain: Tuple (ymin, ymax) defining the domain for 2D functions (set to None for 1D).\n",
    "      - title: Title for the plots.\n",
    "\n",
    "    Returns:\n",
    "      - path: The list of parameter values at each epoch (for further processing if desired).\n",
    "    \"\"\"\n",
    "    # Run the optimization (assumes a separate \"optimize\" function exists).\n",
    "    path = optimize(loss_func, update_func, initial_params, epochs, hyperparams , second_order=second_order)\n",
    "    path_np = np.array(path)  # Convert list of torch tensors (or arrays) to a NumPy array.\n",
    "    initial_np = initial_params.detach().numpy()  # Get initial point as a NumPy array.\n",
    "\n",
    "    # Check dimensionality: if initial_params has shape (1, ...) then it's 1D; if (2, ...) then 2D.\n",
    "    if initial_params.shape[0] == 1:\n",
    "        # 1D case: Pass initial_x and path.\n",
    "        plot_1d_function_torch(\n",
    "            func=loss_func,\n",
    "            domain=x_domain,\n",
    "            global_x=path_np[-1][0],\n",
    "            title=title,\n",
    "            initial_x=initial_np[0],\n",
    "            path=path_np\n",
    "        )\n",
    "        plot_loss_vs_epoch(loss_func, path_np)\n",
    "    elif initial_params.shape[0] == 2:\n",
    "        # 2D case: Pass initial_point as a tuple and the full path.\n",
    "        plot_2d_contour_torch(\n",
    "            func=loss_func,\n",
    "            x_domain=x_domain,\n",
    "            y_domain=y_domain,\n",
    "            global_point=(path_np[-1][0], path_np[-1][1]),\n",
    "            title=title,\n",
    "            initial_point=(initial_np[0], initial_np[1]),\n",
    "            path=path_np\n",
    "        )\n",
    "        plot_interactive_3d_torch(\n",
    "            func=loss_func,\n",
    "            x_domain=x_domain,\n",
    "            y_domain=y_domain,\n",
    "            global_point=(path_np[-1][0], path_np[-1][1]),\n",
    "            title=title,\n",
    "            initial_point=(initial_np[0], initial_np[1]),\n",
    "            path=path_np\n",
    "        )\n",
    "        plot_loss_vs_epoch(loss_func, path_np)\n",
    "    else:\n",
    "        raise ValueError(\"Function must be either 1D or 2D.\")\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NZ84cDiT-efX"
   },
   "outputs": [],
   "source": [
    "def run_all_optimizations(update_func, hyperparams, epochs,second_order=False):\n",
    "    \"\"\"\n",
    "    Run optimization for all benchmark functions (both 1D and 2D) using the provided update function,\n",
    "    hyperparameters, and epoch count. It calls `optimize_and_plot` to handle visualization.\n",
    "\n",
    "    Parameters:\n",
    "      - update_func: The update function to use (e.g., adam_update, sgd_update, etc.).\n",
    "      - hyperparams: Dictionary of hyperparameters for the update function.\n",
    "      - epochs: Number of optimization steps to perform.\n",
    "    \"\"\"\n",
    "    # List of benchmark function configurations.\n",
    "    configs = [\n",
    "        # 1D Functions\n",
    "        {\n",
    "            \"title\": \"1D Rastrigin Function Optimization\",\n",
    "            \"loss_func\": rastrigin_1d_torch,\n",
    "            \"initial_params\": torch.tensor([3.0], dtype=torch.float32, requires_grad=True),\n",
    "            \"x_domain\": (-5, 5),\n",
    "            \"y_domain\": None\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"1D Schwefel Function Optimization\",\n",
    "            \"loss_func\": schwefel_1d_torch,\n",
    "            \"initial_params\": torch.tensor([100.0], dtype=torch.float32, requires_grad=True),\n",
    "            \"x_domain\": (0, 500),\n",
    "            \"y_domain\": None\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"1D Ill-Conditioned Convex Function Optimization\",\n",
    "            \"loss_func\": ill_conditioned_convex_1d_torch,\n",
    "            \"initial_params\": torch.tensor([3.0], dtype=torch.float32, requires_grad=True),\n",
    "            \"x_domain\": (-5, 5),\n",
    "            \"y_domain\": None\n",
    "        },\n",
    "        # 2D Functions\n",
    "        {\n",
    "            \"title\": \"2D Rosenbrock Function Optimization\",\n",
    "            \"loss_func\": rosenbrock_2d_torch,\n",
    "            \"initial_params\": torch.tensor([-1.0, 1.5], dtype=torch.float32, requires_grad=True),\n",
    "            \"x_domain\": (-2, 2),\n",
    "            \"y_domain\": (-1, 3)\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"2D Rastrigin Function Optimization\",\n",
    "            \"loss_func\": rastrigin_2d_torch,\n",
    "            \"initial_params\": torch.tensor([3.0, 3.0], dtype=torch.float32, requires_grad=True),\n",
    "            \"x_domain\": (-5, 5),\n",
    "            \"y_domain\": (-5, 5)\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"2D Ackley Function Optimization\",\n",
    "            \"loss_func\": ackley_2d_torch,\n",
    "            \"initial_params\": torch.tensor([3.0, 3.0], dtype=torch.float32, requires_grad=True),\n",
    "            \"x_domain\": (-5, 5),\n",
    "            \"y_domain\": (-5, 5)\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"2D Beale Function Optimization\",\n",
    "            \"loss_func\": beale_2d_torch,\n",
    "            \"initial_params\": torch.tensor([-3.0, 3.0], dtype=torch.float32, requires_grad=True),\n",
    "            \"x_domain\": (-4.5, 4.5),\n",
    "            \"y_domain\": (-4.5, 4.5)\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"2D Eggholder Function Optimization\",\n",
    "            \"loss_func\": eggholder_2d_torch,\n",
    "            \"initial_params\": torch.tensor([0.0, 0.0], dtype=torch.float32, requires_grad=True),\n",
    "            \"x_domain\": (-512, 512),\n",
    "            \"y_domain\": (-512, 512)\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"2D Ill-Conditioned Convex Function Optimization\",\n",
    "            \"loss_func\": ill_conditioned_convex_2d_torch,\n",
    "            \"initial_params\": torch.tensor([4.0, -3.0], dtype=torch.float32, requires_grad=True),\n",
    "            \"x_domain\": (-5, 5),\n",
    "            \"y_domain\": (-5, 5)\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Iterate over each function and call optimize_and_plot\n",
    "    for config in configs:\n",
    "        print(\"Optimizing:\", config[\"title\"])\n",
    "\n",
    "        optimize_and_plot(\n",
    "            loss_func=config[\"loss_func\"],\n",
    "            update_func=update_func,\n",
    "            initial_params=config[\"initial_params\"],\n",
    "            epochs=epochs,\n",
    "            hyperparams=hyperparams,\n",
    "            x_domain=config[\"x_domain\"],\n",
    "            y_domain=config[\"y_domain\"],\n",
    "            title=config[\"title\"],\n",
    "            second_order=second_order\n",
    "        )\n",
    "\n",
    "        print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PsdVw8NLNFuC"
   },
   "source": [
    "# Implementation of Optimization Algorithms\n",
    "\n",
    "Now that the optimization functions have been implemented, in this section we will implement various optimization algorithms. Some parts of these algorithms need to be implemented by you, while other parts have already been provided.\n",
    "\n",
    "For each algorithm, experiment with different parameters and fine-tune them to achieve the best possible performance. The selection and tuning of these parameters are part of your grade, and the number of times you experiment with various settings is a crucial component of this exercise.\n",
    "\n",
    "After implementing each algorithm, please submit a report detailing your observations and findings. Make sure to document your parameter tuning process and the performance outcomes for each algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RZTlCjY0E8oR"
   },
   "source": [
    "# Stochastic Gradient Descent (SGD)(5 Points)\n",
    "\n",
    "**Mathematical Formula:**\n",
    "\n",
    "$$\n",
    "w_{t+1} = w_t - \\eta \\nabla f(w_t)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\\\(w_t\\\\) is the parameter vector at iteration \\\\(t\\\\),\n",
    "- \\\\(\\eta\\\\) is the learning rate, and\n",
    "- \\\\(\\nabla f(w_t)\\\\) is the gradient of the loss function evaluated at \\\\(w_t\\\\).\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "Stochastic Gradient Descent (SGD) is a fundamental first-order optimization method that updates the parameters in the opposite direction of the gradient. At each iteration, the update rule moves the parameters by a small step proportional to the gradient and the learning rate. This method is widely used because of its simplicity and efficiency, particularly when dealing with large datasets. However, its performance heavily depends on the proper tuning of the learning rate; if set too high, the algorithm might overshoot minima, and if too low, convergence can be very slow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9CR_FpRrsghO"
   },
   "outputs": [],
   "source": [
    "def sgd_update(params, grad, state, hyperparams, **kwargs):\n",
    "    \"\"\"\n",
    "    SGD update function.\n",
    "\n",
    "    Parameters:\n",
    "      - params: Current parameter values (torch tensor).\n",
    "      - grad: Gradient of the loss with respect to the parameters.\n",
    "      - state: Dictionary to store any state (unused in SGD).\n",
    "      - hyperparams: Dictionary of hyperparameters. Must contain \"lr\" (learning rate).\n",
    "      - **kwargs: Additional arguments (ignored for SGD).\n",
    "\n",
    "    Returns:\n",
    "      - new_params: Updated parameters.\n",
    "      - state: Unchanged state (empty dictionary).\n",
    "    \"\"\"\n",
    "    lr = hyperparams[\"lr\"]  # Get the learning rate from hyperparameters.\n",
    "\n",
    "    # TODO: Implement the standard SGD update rule\n",
    "    new_params = params - lr * grad\n",
    "\n",
    "    return new_params, state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wGXN81GJFHIu"
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters for SGD (adjust the learning rate as needed).\n",
    "sgd_hyperparams = {\"lr\": 0.001}\n",
    "\n",
    "# Run optimizations for all benchmark functions using SGD.\n",
    "run_all_optimizations(update_func=sgd_update, hyperparams=sgd_hyperparams, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ESRhsIjkOjYs"
   },
   "source": [
    "# SGD Hyperparameter Analysis(5 Points)\n",
    "\n",
    "For the SGD optimizer, we have defined the initial hyperparameters as follows:\n",
    "\n",
    "    sgd_hyperparams = {\"lr\": 0.01}\n",
    "\n",
    "In this section, you are required to test various learning rates and analyze how they affect the optimizer's performance. Please answer the following questions based on your experiments:\n",
    "\n",
    "1. **Parameter Variation:**\n",
    "After experimenting with different values, we tested the following learning rates for SGD:\n",
    "\n",
    "| Learning Rate | Observation |\n",
    "|---------------|-------------------------------------- |\n",
    "| 0.001         | Very slow convergence, but much better convergence. |\n",
    "| 0.01          | Moderate speed, stable in most cases but diverges in complex 2D ones. |\n",
    "| 0.05          | Faster initial convergence, but overshoots, oscillated and diverges in most cases. |\n",
    "| 0.1           | Similar to 0.05.                |\n",
    "| 0.5           | Almost always oscillates, diverges or fails to converge.                          |\n",
    "\n",
    "2. **Performance Analysis:**\n",
    "   - **Convergence Behavior**: \n",
    "      - **Lower LRs (0.001, 0.01)**: Showed more stable behavior and slower convergence. On simpler 1D functions, they converged successfully, although 0.001 often got stuck or barely moved after a certain point and 0.01 overshooting on some more complex functions.\n",
    "      - **Mid-range LR (0.05)**: Helped the optimizer move quickly at first but caused oscillations or overshooting on functions like the 2D Rosenbrock or Rastrigin.\n",
    "      - **Higher LRs (0.1, 0.5)**: Led to unstable behavior in most 2D tests, often diverging or looping indefinitely without settling.\n",
    "   \n",
    "   - **Unstable/Overshooting**: \n",
    "      - Learning rates of 0.05 and above often caused the optimizer to jump past minima or spiral outwards, especially noticeable in non-convex surfaces with narrow valleys.\n",
    "      \n",
    "   - **Getting Stuck in Local Minima**:\n",
    "      - At 0.001, while it doesn‚Äôt overshoot, it moved so slowly that on functions with many local minima (e.g., 2D Rastrigin, Beale), it often stayed in a suboptimal region, effectively ‚Äústuck‚Äù there due to tiny gradient updates.\n",
    "\n",
    "3. **Optimal Learning Rate:**\n",
    "   - **Best Balance**: From the experiments, **0.001** provided the best overall trade-off between speed and stability. It generally avoided the severe overshooting seen with higher values and converged as fast as 0.01.\n",
    "\n",
    "4. **Trade-offs and Observations:**\n",
    "   - **Higher Learning Rates** (‚â• 0.05): \n",
    "      - **Pros**: Faster initial descent on some simpler functions.  \n",
    "      - **Cons**: High risk of overshooting or failing to converge entirely, especially on ill-conditioned or highly non-convex surfaces.\n",
    "   - **Lower Learning Rates** (‚â§ 0.01): \n",
    "      - **Pros**: More stable updates, less oscillation.  \n",
    "      - **Cons**: Very slow progress, increased likelihood of stagnation in local minima.\n",
    "   - **Emerging Pattern**: Most of the ‚Äúeasier‚Äù 1D functions tolerated a broad range of learning rates. The more complex 2D benchmarks (Rosenbrock, Beale, Rastrigin) displayed significant sensitivity and tended to diverge or oscillate unless the learning rate was moderate.\n",
    "\n",
    "\n",
    "5. **Recommendations:**\n",
    "   - We should definitely start with lower values and then increase them based on the amount of oscillations, overshootings and convergence speed.\n",
    "\n",
    "Make sure your report includes detailed observations, a comparative analysis, and a summary of all the learning rates tested. Your analysis should clearly illustrate how the choice of learning rate influences the performance of the SGD optimizer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1sldK0pfHNBm"
   },
   "source": [
    "# SGD with Momentum (5 Points)\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "\n",
    "The SGD with Momentum update is given by:\n",
    "\n",
    "$$\n",
    "v_t = \\beta v_{t-1} + \\eta \\nabla f(w_t)\n",
    "$$\n",
    "\n",
    "$$\n",
    "w_{t+1} = w_t - v_t\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- \\\\(w_t\\\\) is the parameter vector at iteration \\\\(t\\\\),  \n",
    "- \\\\(v_t\\\\) is the velocity (accumulated momentum) at iteration \\\\(t\\\\),  \n",
    "- \\\\(\\eta\\\\) is the learning rate, and  \n",
    "- \\\\(\\beta\\\\) is the momentum coefficient (typically between 0 and 1).\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "SGD with Momentum introduces an additional term \\\\(v_t\\\\) that accumulates a running average of past gradients. This helps dampen oscillations and accelerates convergence, especially in scenarios where the gradient direction is consistent. However, careful tuning of both the learning rate \\\\(\\eta\\\\) and the momentum coefficient \\\\(\\beta\\\\) is necessary for optimal performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "umXqXtHbF4CJ"
   },
   "outputs": [],
   "source": [
    "def momentum_update(params, grad, state, hyperparams, **kwargs):\n",
    "    \"\"\"\n",
    "    SGD with Momentum update function.\n",
    "\n",
    "    Parameters:\n",
    "      - params: Current parameter values (torch tensor).\n",
    "      - grad: Gradient of the loss with respect to the parameters.\n",
    "      - state: Dictionary to store state (e.g., velocity). Initially, this can be empty.\n",
    "      - hyperparams: Dictionary of hyperparameters. Must contain:\n",
    "          \"lr\": learning rate, and\n",
    "          \"momentum\": momentum coefficient.\n",
    "      - **kwargs: Additional arguments (not used here).\n",
    "\n",
    "    Returns:\n",
    "      - new_params: Updated parameters.\n",
    "      - state: Updated state dictionary (contains the velocity).\n",
    "    \"\"\"\n",
    "    lr = hyperparams[\"lr\"]\n",
    "    momentum = hyperparams[\"momentum\"]\n",
    "\n",
    "    if \"v\" not in state:\n",
    "        # TODO: Initialize the velocity tensor with zeros, having the same shape as params\n",
    "        state[\"v\"] = torch.zeros_like(params)\n",
    "\n",
    "    # TODO: Implement the momentum update rule: v_t = momentum * v_{t-1} + lr * grad\n",
    "    state[\"v\"] = momentum * state[\"v\"] + lr * grad\n",
    "\n",
    "    # TODO: Update parameters using the velocity term\n",
    "    new_params = params - state[\"v\"]\n",
    "\n",
    "    return new_params, state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SnUgXIssF6Ol"
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters for SGD with Momentum.\n",
    "momentum_hyperparams = {\n",
    "    \"lr\": 0.001,       # learning rate\n",
    "    \"momentum\": 0.9    # momentum coefficient\n",
    "}\n",
    "\n",
    "# Run optimizations for all benchmark functions using SGD with Momentum.\n",
    "run_all_optimizations(update_func=momentum_update, hyperparams=momentum_hyperparams, epochs=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AeaRKyEDOubj"
   },
   "source": [
    "# Momentum Hyperparameter Analysis(5 Points)\n",
    "\n",
    "For the Momentum optimizer, the hyperparameters are defined as follows:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "momentum_hyperparams = {\n",
    "    \"lr\": 0.001,       # learning rate\n",
    "    \"momentum\": 0.9    # momentum coefficient\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In this section, you are required to test various values for both the learning rate and the momentum coefficient, and analyze their impact on the optimizer's performance. Please answer the following questions based on your experiments:\n",
    "\n",
    "1. Parameter Variation:\n",
    "   Below is a summary table of the main combinations:\n",
    "\n",
    "| Learning Rate | Momentum | Observations                                               |\n",
    "|---------------|----------|------------------------------------------------------------|\n",
    "| 0.0001        | 0.5      | Very slow but stable; occasionally stuck in local minima.  |\n",
    "| 0.0001        | 0.9      | Slow convergence; fewer oscillations.                      |\n",
    "| 0.0001        | 0.95     | Slight speed-up, still quite slow overall.                |\n",
    "| 0.0001        | 0.99     | Slower, but smoother updates; might not escape some minima.|\n",
    "| 0.001         | 0.5      | Moderately stable; can be a bit slower on rugged landscapes.|\n",
    "| 0.001         | 0.9      | Good overall performance; decent speed + stability.        |\n",
    "| 0.001         | 0.95     | Faster initial progress; occasional overshoot on complex surfaces. |\n",
    "| 0.001         | 0.99     | Can converge quickly if not trapped; some oscillation possible.|\n",
    "| 0.01          | 0.5      | Faster convergence in 1D tests; can overshoot in 2D.       |\n",
    "| 0.01          | 0.9      | Tends to overshoot on ill-conditioned or narrow-valley functions. |\n",
    "| 0.01          | 0.95     | Rapid movement initially, leading to instability on functions like Beale. |\n",
    "| 0.01          | 0.99     | Frequent strong oscillations; prone to divergence in complex landscapes. |\n",
    "| 0.1           | 0.9      | Almost always diverges quickly on tricky functions.        |\n",
    "| 0.1           | 0.95     | Very unstable; partial or complete divergence.            |\n",
    "| 0.1           | 0.99     | Highly unstable in most cases.                             |\n",
    "\n",
    "2. Performance Analysis:\n",
    "   - **Learning Rate Effects**:\n",
    "      - **Lower LRs (‚â§ 0.0001 or 0.001)**: \n",
    "         - Yielded stable updates and less oscillation, but sometimes converged too slowly (or got stuck in local minima).\n",
    "         - Often needed more epochs to escape rugged areas.\n",
    "      - **Mid to High LRs (‚â• 0.01)**:\n",
    "         - Offered faster initial descent on simpler 1D functions.\n",
    "         - Caused overshooting or even divergence on complex 2D functions like Rosenbrock, Beale, and Rastrigin.\n",
    "   - **Momentum Coefficient Variations**:\n",
    "      - **Lower Momentum (0.5)**:\n",
    "         - Reduced the ‚Äúboost‚Äù effect, so updates remained smaller and more controlled.\n",
    "         - Convergence was steadier but often slower, especially in wide plateaus.\n",
    "      - **Higher Momentum (‚â• 0.9)**:\n",
    "         - Accelerated progress along consistent gradient directions, often improving initial convergence speed.\n",
    "         - Could cause oscillations or overshooting in narrow valleys or sharply curved regions (e.g., 2D Rosenbrock).\n",
    "         - Very high momentum (0.99) risked buildup of large ‚Äúvelocity‚Äù terms, making the optimizer difficult to control on difficult surfaces.\n",
    "   - **Faster Convergence vs. Instability**:\n",
    "      - **Combinations with LR ~ 0.001 and momentum ~ 0.9 or 0.95** struck a good balance on most benchmarks, converging relatively quickly without severe divergence.\n",
    "      - **High LR + high momentum** often led to extreme oscillation or divergence, especially evident in Beale and ill-conditioned functions.\n",
    "\n",
    "\n",
    "3. Optimal Parameter Combination:\n",
    "   - **Best Combination**: Across the tests, **LR = 0.001** and **momentum = 0.9** provided the most consistent results‚Äîgood convergence speed on simpler landscapes while staying stable on more complex ones.\n",
    "   - **Why Optimal?**:\n",
    "      - This parameter set typically avoided the slow crawling of extremely low LR or the instability of higher LR values.\n",
    "      - The momentum of 0.9 accelerated the optimizer sufficiently without causing uncontrollable oscillations in most cases.\n",
    "\n",
    "\n",
    "4. Trade-offs and Observations:\n",
    "   - **Higher Momentum**:\n",
    "      - **Pros**: Faster descent along well-defined gradient directions; can ‚Äúpush through‚Äù minor local traps.\n",
    "      - **Cons**: More prone to overshoot in curved valleys; might cycle or diverge if combined with a high LR.\n",
    "   - **Lower Momentum**:\n",
    "      - **Pros**: More controlled, stable updates; less risk of large oscillations.\n",
    "      - **Cons**: Slower progress, especially on functions with broad plateaus or shallow gradients.\n",
    "   - **Learning Rate Trends**:\n",
    "      - Big steps (‚â• 0.01) can boost speed on simpler functions but risk divergence on complex ones.\n",
    "      - Tiny steps (‚â§ 0.0001) generally avoid overshooting but converge very slowly or stall in local minima.\n",
    "\n",
    "\n",
    "5. Recommendations:\n",
    "   - Quite similar to the previous one, we should start with lower learning rates, then we can change the momentum in a way to minimize the oscillations and divergences. Then we can slowly start to use higher learning rates.\n",
    "\n",
    "Ensure your report includes detailed observations, a comparative analysis, and a summary of all the hyperparameter values tested. Your analysis should clearly illustrate how the choice of learning rate and momentum coefficient influences the performance of the Momentum optimizer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7QYRrs-VJN1c"
   },
   "source": [
    "# Nesterov Accelerated Gradient (NAG)\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "\n",
    "The Nesterov Accelerated Gradient update is given by:\n",
    "\n",
    "$$\n",
    "v_t = \\beta \\, v_{t-1} + \\eta \\, \\nabla f(w_t)\n",
    "$$\n",
    "\n",
    "$$\n",
    "w_{t+1} = w_t - \\beta \\, v_{t-1} - \\eta \\, \\nabla f(w_t)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\\\( w_t \\\\) is the parameter vector at iteration \\\\( t \\\\),\n",
    "- \\\\( v_t \\\\) is the velocity (accumulated momentum) at iteration \\\\( t \\\\),\n",
    "- \\\\( \\eta \\\\) is the learning rate, and\n",
    "- \\\\( \\beta \\\\) is the momentum coefficient (typically a value between 0 and 1).\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "Nesterov Accelerated Gradient (NAG) is a variant of SGD with momentum that computes the gradient at a \"lookahead\" position. Instead of computing the gradient at the current parameter \\\\( w_t \\\\) only, NAG uses the accumulated momentum from previous iterations to ‚Äúlook ahead‚Äù before updating the parameters. The update can be interpreted as:\n",
    "\n",
    "1. **Compute the momentum update:**  \n",
    "   \\\\( v_t = \\beta \\, v_{t-1} + \\eta \\, \\nabla f(w_t) \\\\)\n",
    "2. **Update the parameters using the previous momentum:**  \n",
    "   \\\\( w_{t+1} = w_t - \\beta \\, v_{t-1} - \\eta \\, \\nabla f(w_t) \\\\)\n",
    "\n",
    "This \"lookahead\" strategy often leads to faster convergence compared to standard momentum because it anticipates the effect of momentum and adjusts the update accordingly.\n",
    "\n",
    "*Note:* In our implementation, we assume that we cannot recompute the gradient at the lookahead position (for simplicity) so we approximate NAG with the following update:\n",
    "\n",
    "$$\n",
    "v_{\\text{prev}} = v_{t-1} \\quad,\\quad v_t = \\beta \\, v_{t-1} + \\eta \\, \\nabla f(w_t)\n",
    "$$\n",
    "\n",
    "$$\n",
    "w_{t+1} = w_t - \\beta \\, v_{\\text{prev}} - \\eta \\, \\nabla f(w_t)\n",
    "$$\n",
    "\n",
    "This formulation uses the previous momentum term directly for the \"lookahead\" component.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WDQvhoyArOw4"
   },
   "outputs": [],
   "source": [
    "def nag_update(params, grad, state, hyperparams, **kwargs):\n",
    "    \"\"\"\n",
    "    Nesterov Accelerated Gradient (NAG) update function.\n",
    "\n",
    "    Parameters:\n",
    "      - params: Current parameter values (torch tensor).\n",
    "      - grad: Gradient of the loss with respect to params.\n",
    "      - state: Dictionary to store the momentum (velocity). Initially empty.\n",
    "      - hyperparams: Dictionary of hyperparameters. Must contain:\n",
    "            \"lr\": learning rate, and\n",
    "            \"momentum\": momentum coefficient.\n",
    "      - **kwargs: Additional arguments (ignored for NAG).\n",
    "\n",
    "    Returns:\n",
    "      - new_params: Updated parameter values.\n",
    "      - state: Updated state dictionary containing the velocity.\n",
    "    \"\"\"\n",
    "    lr = hyperparams[\"lr\"]\n",
    "    momentum = hyperparams[\"momentum\"]\n",
    "\n",
    "    # Initialize the velocity if not already in state.\n",
    "    if \"v\" not in state:\n",
    "        state[\"v\"] = torch.zeros_like(params)\n",
    "\n",
    "    # Save the previous velocity for the lookahead.\n",
    "    v_prev = state[\"v\"].clone()\n",
    "    # Update the velocity.\n",
    "    state[\"v\"] = momentum * state[\"v\"] + lr * grad\n",
    "    # Update the parameters using the lookahead formula.\n",
    "    new_params = params - momentum * v_prev - lr * grad\n",
    "    return new_params, state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uUmOpdw9GLU2"
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters for Nesterov Accelerated Gradient.\n",
    "nag_hyperparams = {\n",
    "    \"lr\": 0.001,        # Learning rate\n",
    "    \"momentum\": 0.9     # Momentum coefficient\n",
    "}\n",
    "\n",
    "# Run optimizations for all benchmark functions using NAG.\n",
    "run_all_optimizations(update_func=nag_update, hyperparams=nag_hyperparams, epochs=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dNFNBxUSQvOe"
   },
   "source": [
    "# NAG Hyperparameter Analysis(5 Points)\n",
    "\n",
    "For the Nesterov Accelerated Gradient (NAG) optimizer, we have defined the initial hyperparameters as follows:\n",
    "\n",
    "    nag_hyperparams = {\n",
    "        \"lr\": 0.001,        # Learning rate\n",
    "        \"momentum\": 0.9     # Momentum coefficient\n",
    "    }\n",
    "\n",
    "In this section, you are required to test various learning rates and momentum values to analyze how they affect the performance of NAG. Please answer the following questions based on your experiments:\n",
    "\n",
    "1. **Parameter Variation:**\n",
    "   - Similar to the momentum-based experiments, we explored several learning rates and momentum coefficients:\n",
    "\n",
    "| Learning Rate | Momentum | Observations                                               |\n",
    "|---------------|----------|------------------------------------------------------------|\n",
    "| 0.0001        | 0.5      | Very slow, stable; minimal overshoot, but may stall.       |\n",
    "| 0.0001        | 0.9      | Slower but smoother, especially on non-convex surfaces.    |\n",
    "| 0.0001        | 0.95     | Still very slow, though it helps push through minor dips.  |\n",
    "| 0.0001        | 0.99     | Smooth updates; might remain stuck if gradients are tiny.  |\n",
    "| 0.001         | 0.5      | Moderate speed; little risk of major oscillations.         |\n",
    "| 0.001         | 0.9      | Generally stable, relatively fast convergence.            |\n",
    "| 0.001         | 0.95     | Faster initial movement; occasional overshoot on complex landscapes. |\n",
    "| 0.001         | 0.99     | Can speed up but risks bigger swings in sharp valleys.     |\n",
    "| 0.01          | 0.5      | Noticeably faster on simpler functions; minor risk of overshoot.  |\n",
    "| 0.01          | 0.9      | Potentially unstable on highly curved surfaces (e.g., Beale).  |\n",
    "| 0.01          | 0.95     | Fast initial progress; can diverge or oscillate if gradients are steep. |\n",
    "| 0.01          | 0.99     | Strong, persistent oscillations in complex regions.        |\n",
    "| 0.1           | 0.9      | Often diverges in most 2D non-convex functions.           |\n",
    "| 0.1           | 0.95     | Very unstable; large swings in function value.            |\n",
    "| 0.1           | 0.99     | Extremely prone to divergence.                             |\n",
    "\n",
    "2. **Performance Analysis:**\n",
    "   - **Impact of Learning Rate**:\n",
    "      - **Lower LRs (‚â§ 0.0001 or 0.001)**: More controlled progress, fewer oscillations or divergences, but can be slow on functions with wide plateaus or many local minima.\n",
    "      - **Higher LRs (‚â• 0.01)**: Much quicker descent initially, but prone to overshoot or diverge in narrow valleys and ill-conditioned functions (e.g., Rosenbrock).\n",
    "\n",
    "   - **Momentum Coefficient Variations**:\n",
    "      - **Lower Momentum (0.5)**: Provides more gradual acceleration, resulting in steadier but slower convergence.  \n",
    "      - **Higher Momentum (‚â• 0.9)**: Speeds up progress in consistent gradient directions. However, on tricky 2D functions, it can overshoot or oscillate if combined with a larger LR.\n",
    "      - **NAG‚Äôs ‚ÄúLookahead‚Äù**: Tends to reduce some overshooting compared to standard momentum but can still oscillate if LR and momentum are both set too high.\n",
    "\n",
    "   - **Instabilities and Local Minima**:\n",
    "      - Overshooting and partial divergence were most common with LR‚â•0.01 and momentum‚â•0.9, especially on Beale and other multi-modal surfaces.\n",
    "      - Extremely low LR (e.g., 0.0001) occasionally led to the optimizer sitting in local minima too long, particularly on functions with rugged landscapes.\n",
    "\n",
    "3. **Optimal Parameters:**\n",
    "   - **Best Balance**: **Learning Rate = 0.001** and **Momentum = 0.9** emerged again as a solid compromise.  \n",
    "   - **Why Optimal?**: \n",
    "      - Consistently avoided severe divergence or chaotic oscillations on the more complex tests.\n",
    "      - Provided reasonably fast convergence without requiring excessive tuning across different functions.\n",
    "\n",
    "4. **Trade-offs and Observations:**\n",
    "   - **Higher Momentum**:\n",
    "      - **Pros**: Accelerates the search, helps skip over small bumps.  \n",
    "      - **Cons**: If combined with a moderately high LR, it can produce strong oscillations or divergence.\n",
    "   - **Lower Momentum**:\n",
    "      - **Pros**: More stable, fewer swings.  \n",
    "      - **Cons**: Slower to leave plateaus and not as efficient in deeper valleys.\n",
    "   - **Key Trend**: NAG‚Äôs lookahead generally helps reduce some overshooting issues seen in basic momentum, but extreme parameter values (high LR + high momentum) still risk instability.\n",
    "\n",
    "5. **Recommendations:**\n",
    "   - Similar to SGD + Momentum\n",
    "\n",
    "Ensure your report includes detailed observations, a comparative analysis, and a summary of all parameter combinations tested. Your analysis should clearly illustrate how the choice of these hyperparameters influences the performance of the NAG optimizer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IRT-aFgZKa8F"
   },
   "source": [
    "# Adagrad (Adaptive Gradient)\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "\n",
    "Adagrad adapts the learning rate for each parameter individually by accumulating the squared gradients. Its update equations are:\n",
    "\n",
    "$$\n",
    "G_t = G_{t-1} + \\left(\\nabla f(w_t)\\right)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "w_{t+1} = w_t - \\frac{\\eta}{\\sqrt{G_t} + \\epsilon} \\, \\nabla f(w_t)\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- \\\\(w_t\\\\) is the parameter vector at iteration \\\\(t\\\\),  \n",
    "- \\\\(G_t\\\\) is the accumulation of squared gradients (applied elementwise),  \n",
    "- \\\\(\\eta\\\\) is the learning rate, and  \n",
    "- $\\epsilon$ is a small constant added for numerical stability.\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "Adagrad adjusts the learning rate based on the history of gradients. Parameters with high gradients accumulate a larger \\\\(G_t\\\\) and thus receive a smaller effective learning rate, while parameters with small or sparse gradients receive larger updates. This makes Adagrad particularly useful for problems with sparse features, but note that its learning rate can decay too aggressively over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "laNX8UNNrOw5"
   },
   "outputs": [],
   "source": [
    "def adagrad_update(params, grad, state, hyperparams, **kwargs):\n",
    "    \"\"\"\n",
    "    Adagrad update function.\n",
    "\n",
    "    Parameters:\n",
    "      - params: Current parameter values (torch tensor).\n",
    "      - grad: Gradient of the loss with respect to the parameters.\n",
    "      - state: Dictionary to store state (e.g., accumulated squared gradients).\n",
    "      - hyperparams: Dictionary of hyperparameters. Must contain:\n",
    "          \"lr\": learning rate, and\n",
    "          \"eps\": a small constant for numerical stability.\n",
    "      - **kwargs: Additional arguments (ignored for Adagrad).\n",
    "\n",
    "    Returns:\n",
    "      - new_params: Updated parameters.\n",
    "      - state: Updated state dictionary.\n",
    "    \"\"\"\n",
    "    lr = hyperparams[\"lr\"]\n",
    "    eps = hyperparams[\"eps\"]\n",
    "\n",
    "    if \"G\" not in state:\n",
    "        state[\"G\"] = torch.zeros_like(params)\n",
    "\n",
    "    # Accumulate squared gradients.\n",
    "    state[\"G\"] += grad**2\n",
    "\n",
    "    # Perform the Adagrad update.\n",
    "    new_params = params - lr * grad / (torch.sqrt(state[\"G\"]) + eps)\n",
    "    return new_params, state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4lzeCmIFGwMN"
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters for Adagrad.\n",
    "adagrad_hyperparams = {\n",
    "    \"lr\": 0.01,   # Learning rate\n",
    "    \"eps\": 1e-7   # Small constant for numerical stability\n",
    "}\n",
    "\n",
    "# Run optimizations for all benchmark functions using Adagrad.\n",
    "run_all_optimizations(update_func=adagrad_update, hyperparams=adagrad_hyperparams, epochs=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Jz2gthSQ2cm"
   },
   "source": [
    "# Adagrad Hyperparameter Analysis(5 Points)\n",
    "\n",
    "For the Adagrad optimizer, we have defined the initial hyperparameters as follows:\n",
    "\n",
    "    adagrad_hyperparams = {\n",
    "        \"lr\": 0.01,   # Learning rate\n",
    "        \"eps\": 1e-8   # Small constant for numerical stability\n",
    "    }\n",
    "\n",
    "In this section, you are required to test various learning rates and epsilon values to analyze how they affect the performance of Adagrad. Please answer the following questions based on your experiments:\n",
    "\n",
    "1. **Parameter Variation:**\n",
    "   - I tested a range of learning rates and epsilon ($\\epsilon$) values:\n",
    "\n",
    "| Learning Rate | Epsilon | Observations                                                             |\n",
    "|---------------|---------|--------------------------------------------------------------------------|\n",
    "| 0.001         | 1e-8    | Very slow but stable; can stall on rugged or multi-modal functions.      |\n",
    "| 0.001         | 1e-7    | Slightly faster than with 1e-8, still rather conservative updates.       |\n",
    "| 0.001         | 1e-6    | Generally stable; no serious overshoot but might not fully converge in 100 epochs. |\n",
    "| 0.001         | 1e-4    | Improved speed, occasional risk of partial overshoot on certain 2D tests.|\n",
    "| 0.01          | 1e-8    | Good balance of speed and stability on most functions.                   |\n",
    "| 0.01          | 1e-7    | Very similar performance to 1e-8, converges slightly faster in some cases.|\n",
    "| 0.01          | 1e-6    | Faster convergence; minor oscillations on complex landscapes.            |\n",
    "| 0.01          | 1e-4    | Risk of overshoot in non-convex or ill-conditioned surfaces.             |\n",
    "| 0.1           | 1e-8    | High risk of oscillations or divergence, especially on 2D complex functions. |\n",
    "| 0.1           | 1e-7    | Similar to above; frequent overshooting.                                 |\n",
    "| 0.1           | 1e-6    | Rapid updates; can converge quickly on easy functions, diverge on tricky ones.|\n",
    "| 0.1           | 1e-4    | Most prone to instability or partial divergence.                          |\n",
    "\n",
    "\n",
    "2. **Performance Analysis:**\n",
    "   - **Learning Rate Effects**:\n",
    "      - **Low LR (0.001)**: Very stable updates, but slow convergence, especially on functions with broad flat regions or numerous local minima.\n",
    "      - **Moderate LR (0.01)**: Balanced speed and stability, generally converging on most functions without extreme oscillations.\n",
    "      - **High LR (0.1)**: Can cause overshooting or even divergence, especially on highly non-convex or ill-conditioned surfaces.\n",
    "\n",
    "   - **Epsilon ($\\epsilon$) Variations**:\n",
    "      - **Very Small $\\epsilon$ (1e-8)**: Provides the original flavor of Adagrad with minimal numerical influence, typically stable but can be slow if gradients become very small.\n",
    "      - **Larger $\\epsilon$ (‚â•1e-6)**: Can help keep updates from shrinking too fast in later stages, leading to faster movement but occasionally introducing minor oscillations.\n",
    "\n",
    "   - **Unstable Behavior / Local Minima**:\n",
    "      - Some overshoot at **LR=0.1** combined with moderate-to-large $\\epsilon$ values, especially for complicated surfaces like 2D Rastrigin or Eggholder.\n",
    "      - At **very low LR (0.001)**, the optimizer can essentially plateau in local minima if the accumulated gradient scaling quickly shrinks step sizes.\n",
    "\n",
    "\n",
    "3. **Optimal Parameters:**\n",
    "   - **Best Combination**: A **learning rate around 0.01** with an **$\\epsilon$ of about 1e-7 or 1e-8** often provided the best trade-off between convergence speed and stability across various benchmarks.\n",
    "   - **Reasoning**: \n",
    "   - At LR=0.01, Adagrad updates aren‚Äôt so large that they overshoot, yet they remain large enough for adequate exploration.\n",
    "   - A slightly smaller $\\epsilon$ (1e-8 or 1e-7) ensures that the step-size reduction mechanism remains significant without causing too much numerical dampening.\n",
    "\n",
    "\n",
    "4. **Trade-offs and Observations:**\n",
    "   - **Lower $\\epsilon$**:\n",
    "      - **Pros**: Keeps Adagrad‚Äôs adaptive mechanism intact for stronger diminishing steps over time, leading to stable convergence.\n",
    "      - **Cons**: May reduce step sizes very quickly, slowing final convergence or getting stuck in local minima.\n",
    "   - **Higher $\\epsilon$**:\n",
    "      - **Pros**: Helps maintain moderately sized updates longer, preventing the optimizer from stalling.\n",
    "      - **Cons**: Can introduce small oscillations or overshooting on sharply curved functions.\n",
    "   - **Learning Rate Trends**:\n",
    "      - A moderate LR avoids both the ‚Äúforever stuck‚Äù scenario of very small LR and the instability of excessively large LR.\n",
    "      - On simpler 1D surfaces, even a slightly higher LR can be fine, but on more complex 2D problems, it can cause divergence.\n",
    "\n",
    "5. **Recommendations:**\n",
    "   - In simples terms, we can start with something like 0.01 as the learning rate and 1e-7 for the epsilon. Then can start changing the epsilon base on whether the models is stalling, overshooting, diverging, etc. Also slightly smaller learning rates are better for more complex function (to ensure that we won't overshoot or diverge).\n",
    "\n",
    "Ensure your report includes detailed observations, a comparative analysis, and a summary of all parameter combinations tested. Your analysis should clearly illustrate how the choice of these hyperparameters influences the performance of the Adagrad optimizer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-ZHeL0ELAkY"
   },
   "source": [
    "# RMSprop (Root Mean Square Propagation)\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "\n",
    "RMSprop adapts the learning rate for each parameter by maintaining an exponentially decaying average of squared gradients. The update equations are:\n",
    "\n",
    "$$\n",
    "E[g^2]_t = \\beta \\, E[g^2]_{t-1} + (1 - \\beta) \\, \\left(\\nabla f(w_t)\\right)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "w_{t+1} = w_t - \\frac{\\eta}{\\sqrt{E[g^2]_t} + \\epsilon} \\, \\nabla f(w_t)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\\\(w_t\\\\) is the parameter vector at iteration \\\\(t\\\\),\n",
    "- \\\\(E[g^2]_t\\\\) is the exponentially decaying average of squared gradients,\n",
    "- \\\\(\\eta\\\\) is the learning rate,\n",
    "- \\\\(\\beta\\\\) is the decay rate (typically around 0.9), and\n",
    "- $\\epsilon$ is a small constant added for numerical stability.\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "RMSprop is designed to overcome the aggressive decay of the learning rate seen in Adagrad by using an exponential decay factor \\\\(\\beta\\\\) to forget old gradients. This helps to maintain a more stable and adaptive learning rate during training. RMSprop works well in practice on non-stationary problems and deep networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2USzXZaeG5x8"
   },
   "outputs": [],
   "source": [
    "def rmsprop_update(params, grad, state, hyperparams, **kwargs):\n",
    "    \"\"\"\n",
    "    RMSprop update function.\n",
    "\n",
    "    Parameters:\n",
    "      - params: Current parameter values (torch tensor).\n",
    "      - grad: Gradient of the loss with respect to parameters.\n",
    "      - state: Dictionary to store state (e.g., running average of squared gradients).\n",
    "      - hyperparams: Dictionary of hyperparameters. Must contain:\n",
    "            \"lr\": learning rate,\n",
    "            \"beta\": decay rate, and\n",
    "            \"eps\": a small constant for numerical stability.\n",
    "      - **kwargs: Additional arguments (ignored for RMSprop).\n",
    "\n",
    "    Returns:\n",
    "      - new_params: Updated parameters.\n",
    "      - state: Updated state dictionary.\n",
    "    \"\"\"\n",
    "    lr = hyperparams[\"lr\"]\n",
    "    beta = hyperparams[\"beta\"]\n",
    "    eps = hyperparams[\"eps\"]\n",
    "\n",
    "    if \"E\" not in state:\n",
    "        state[\"E\"] = torch.zeros_like(params)\n",
    "\n",
    "    # Update the running average of squared gradients.\n",
    "    state[\"E\"] = beta * state[\"E\"] + (1 - beta) * grad**2\n",
    "\n",
    "    # Compute the RMSprop update.\n",
    "    new_params = params - lr * grad / (torch.sqrt(state[\"E\"]) + eps)\n",
    "    return new_params, state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PG1FUJg7G7AS"
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters for RMSprop.\n",
    "rmsprop_hyperparams = {\n",
    "    \"lr\": 0.001,    # Learning rate\n",
    "    \"beta\": 0.9,    # Decay rate\n",
    "    \"eps\": 1e-8     # Small constant for numerical stability\n",
    "}\n",
    "\n",
    "# Run optimizations for all benchmark functions using RMSprop.\n",
    "run_all_optimizations(update_func=rmsprop_update, hyperparams=rmsprop_hyperparams, epochs=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNV6AQIzRFlK"
   },
   "source": [
    "# RMSprop Hyperparameter Analysis(5 Points)\n",
    "\n",
    "For the RMSprop optimizer, we have defined the initial hyperparameters as follows:\n",
    "\n",
    "    rmsprop_hyperparams = {\n",
    "        \"lr\": 0.001,    # Learning rate\n",
    "        \"beta\": 0.9,    # Decay rate\n",
    "        \"eps\": 1e-8     # Small constant for numerical stability\n",
    "    }\n",
    "\n",
    "In this section, you are required to test various learning rates, beta values, and epsilon values to analyze how they affect the performance of RMSprop. Please answer the following questions based on your experiments:\n",
    "\n",
    "1. **Parameter Variation:**\n",
    "   - I experimented with a few ranges for each hyperparameter:\n",
    "\n",
    "| Learning Rate | Beta  | Epsilon | Observations                                                              |\n",
    "|---------------|-------|---------|---------------------------------------------------------------------------|\n",
    "| 0.0001        | 0.9   | 1e-8    | Very slow, stable updates; might not reach deeper minima within 1000 epochs. |\n",
    "| 0.0001        | 0.9   | 1e-6    | Slightly faster than 1e-8, still limited progress in complex landscapes.   |\n",
    "| 0.0001        | 0.9   | 1e-4    | Gradually converges on simpler functions but stalls in multi-modal ones.  |\n",
    "| 0.0001        | 0.99  | 1e-8    | Extremely slow adaptation; minimal oscillation, sometimes stuck in local minima. |\n",
    "| 0.001         | 0.9   | 1e-8    | Balanced speed, moderate success on a variety of functions.               |\n",
    "| 0.001         | 0.9   | 1e-6    | Slight oscillations on tricky 2D surfaces, still relatively stable.       |\n",
    "| 0.001         | 0.9   | 1e-4    | Faster updates but can overshoot in steep regions.                        |\n",
    "| 0.001         | 0.99  | 1e-8    | Very smooth updates, sometimes too slow on rugged functions.             |\n",
    "| 0.001         | 0.99  | 1e-6    | Helpful in smoothing out noise but can converge slowly.                   |\n",
    "| 0.01          | 0.9   | 1e-8    | Quick initial descent; prone to overshooting or oscillation on difficult functions. |\n",
    "| 0.01          | 0.9   | 1e-6    | Faster adaptation, but can get unstable or stuck in local minima.         |\n",
    "| 0.01          | 0.99  | 1e-8    | Large decay, slow parameter updates if gradients are not consistent.      |\n",
    "| 0.01          | 0.99  | 1e-6    | High likelihood of overshooting and diverging on ill-conditioned surfaces.|\n",
    "\n",
    "\n",
    "2. **Performance Analysis:**\n",
    "   - **Learning Rate (LR)**:\n",
    "      - **0.0001**: Very conservative; stable updates but insufficient movement in 1000 epochs for complex or multi-modal functions.\n",
    "      - **0.001**: Struck a reasonable balance of speed and control, generally avoiding major oscillations while still exploring.\n",
    "      - **0.01**: Tends to move quickly but frequently overshoots on rugged 2D benchmarks, sometimes failing to settle into global minima.\n",
    "\n",
    "   - **Beta (Decay Rate)**:\n",
    "      - **0.9**: Provides a moderate smoothing of gradient variance. Often yields quicker adaptation while keeping some agility to respond to new gradient information.\n",
    "      - **0.99**: Stronger smoothing effect can slow changes significantly. While it reduces oscillations further, it can cause the optimizer to ‚Äúcreep‚Äù toward minima and struggle with highly non-convex terrains.\n",
    "\n",
    "   - **Epsilon ($\\epsilon$)**:\n",
    "      - **1e-8**: Minimal baseline to prevent division by zero; stable but can cause very small updates if the squared gradient accumulates a lot.\n",
    "      - **1e-6 and 1e-4**: Allows slightly larger effective step sizes when gradients are small, potentially improving convergence speed but sometimes introducing minor overshoots.\n",
    "\n",
    "   - **Unstable or Slow Convergence**:\n",
    "      - High LR (0.01) with high beta (0.99) was a frequent culprit for oscillation or partial divergence, especially on 2D Rastrigin or Eggholder.\n",
    "      - Very low LR (0.0001) with high beta (0.99) caused extremely slow progress, sometimes trapping the optimizer in suboptimal minima.\n",
    "\n",
    "3. **Optimal Parameters:**\n",
    "   - **Best Combination**: **LR = 0.001, Beta = 0.9, Epsilon = 1e-8** generally worked well across various functions, offering stability with moderate convergence speed.\n",
    "   - **Reasoning**: \n",
    "      - 0.001 is often a sweet spot for RMSprop, neither too large to overshoot nor too small to stall.\n",
    "      - Beta=0.9 smooths out gradient variance without dragging updates too slowly.\n",
    "      - 1e-8 ensures numerical stability without altering the adaptive update too aggressively.\n",
    "\n",
    "\n",
    "4. **Trade-offs and Observations:**\n",
    "   - **Higher Beta**:\n",
    "      - **Pros**: More damping of rapid gradient changes, reducing oscillations.\n",
    "      - **Cons**: Can slow adaptation significantly in rugged or shifting landscapes.\n",
    "   - **Lower Beta**:\n",
    "      - **Pros**: More responsive to changes in the gradient, faster in early iterations.\n",
    "      - **Cons**: Might see slightly more oscillation or sensitivity to local gradient noise.\n",
    "   - **Epsilon Variations**:\n",
    "      - A larger $\\epsilon$ (1e-6 or 1e-4) may help avoid extremely small step sizes late in training, but can introduce extra bounce in the updates.\n",
    "   - **Patterns**:\n",
    "      - RMSprop typically stabilizes more quickly than basic SGD or momentum, but may plateau, failing to fully optimize the most challenging multi-modal functions unless carefully tuned.\n",
    "\n",
    "5. **Recommendations:**\n",
    "   - First, you can start with something like, 0.001 as the learning rate, 0.9 for the beta and 1e-7 for the epsilon. Then you can increase or decrease the beta if you have oscillations or slow convergence speed.\n",
    "\n",
    "Ensure your report includes detailed observations, a comparative analysis, and a summary of all parameter combinations tested. Your analysis should clearly illustrate how the choice of these hyperparameters influences the performance of the RMSprop optimizer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bx1B75YsHD5-"
   },
   "source": [
    "# Adam (Adaptive Moment Estimation)(10 Points)\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "\n",
    "Adam computes two exponential moving averages of the gradients:\n",
    "\n",
    "- **First moment (mean):**\n",
    "\n",
    "$$\n",
    "m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) \\nabla f(w_t)\n",
    "$$\n",
    "\n",
    "- **Second moment (uncentered variance):**\n",
    "\n",
    "$$\n",
    "v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) \\left(\\nabla f(w_t)\\right)^2\n",
    "$$\n",
    "\n",
    "Bias corrections are then applied:\n",
    "\n",
    "$$\n",
    "\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n",
    "$$\n",
    "\n",
    "Finally, the parameter update is given by:\n",
    "\n",
    "$$\n",
    "w_{t+1} = w_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\, \\hat{m}_t\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\\\(w_t\\\\) is the parameter vector at iteration \\\\(t\\\\),\n",
    "- \\\\(\\eta\\\\) is the learning rate,\n",
    "- \\\\(\\beta_1\\\\) and \\\\(\\beta_2\\\\) are the exponential decay rates for the moment estimates, and\n",
    "- $\\epsilon$ is a small constant to avoid division by zero.\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "Adam combines ideas from momentum and RMSprop. By maintaining an exponentially decaying average of past gradients (first moment) and past squared gradients (second moment) along with bias correction, Adam adapts the learning rate for each parameter. This makes it especially suitable for problems with noisy or sparse gradients, as it works well \"out of the box\" with minimal tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_1ygVwm-G8Gc"
   },
   "outputs": [],
   "source": [
    "def adam_update(params, grad, state, hyperparams, **kwargs):\n",
    "    \"\"\"\n",
    "    Adam update function.\n",
    "\n",
    "    Parameters:\n",
    "      - params: Current parameter values (torch tensor).\n",
    "      - grad: Gradient of the loss with respect to params.\n",
    "      - state: Dictionary to store moving averages for first and second moments.\n",
    "      - hyperparams: Dictionary of hyperparameters. Must contain:\n",
    "            \"lr\": learning rate,\n",
    "            \"beta1\": decay rate for the first moment,\n",
    "            \"beta2\": decay rate for the second moment,\n",
    "            \"eps\": a small constant for numerical stability.\n",
    "      - **kwargs: Additional arguments (ignored for Adam).\n",
    "\n",
    "    Returns:\n",
    "      - new_params: Updated parameter values.\n",
    "      - state: Updated state dictionary.\n",
    "    \"\"\"\n",
    "    lr = hyperparams[\"lr\"]\n",
    "    beta1 = hyperparams[\"beta1\"]\n",
    "    beta2 = hyperparams[\"beta2\"]\n",
    "    eps = hyperparams[\"eps\"]\n",
    "\n",
    "    if \"m\" not in state:\n",
    "        # TODO: Initialize first moment (m), second moment (v), and timestep (t)\n",
    "        state[\"m\"] = torch.zeros_like(params)\n",
    "        state[\"v\"] = torch.zeros_like(params)\n",
    "        state[\"t\"] = 0\n",
    "\n",
    "    # TODO: Update timestep\n",
    "    state[\"t\"] += 1\n",
    "\n",
    "    # TODO: Compute biased first moment estimate (m) and second moment estimate (v)\n",
    "    state[\"m\"] = beta1 * state[\"m\"] + (1 - beta1) * grad\n",
    "    state[\"v\"] = beta2 * state[\"v\"] + (1 - beta2) * grad ** 2\n",
    "\n",
    "    # TODO: Compute bias-corrected first moment estimate (m_hat) and second moment estimate (v_hat)\n",
    "    m_hat = state[\"m\"] / (1 - beta1 ** state[\"t\"])\n",
    "    v_hat = state[\"v\"] / (1 - beta2 ** state[\"t\"])\n",
    "\n",
    "    # TODO: Update parameters using Adam's rule\n",
    "    new_params = params - (lr * m_hat) / (torch.sqrt(v_hat) + eps)\n",
    "\n",
    "    return new_params, state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rLTL4r2tHFj9"
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters for Adam.\n",
    "adam_hyperparams = {\n",
    "    \"lr\": 0.001,      # Learning rate\n",
    "    \"beta1\": 0.9,     # Decay rate for first moment\n",
    "    \"beta2\": 0.999,   # Decay rate for second moment\n",
    "    \"eps\": 1e-8       # Smoothing constant\n",
    "}\n",
    "\n",
    "# Run optimizations for all benchmark functions using Adam.\n",
    "run_all_optimizations(update_func=adam_update, hyperparams=adam_hyperparams, epochs=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBOnUPf5RTpR"
   },
   "source": [
    "# Adam Hyperparameter Analysis(5 Points)\n",
    "\n",
    "For the Adam optimizer, we have defined the initial hyperparameters as follows:\n",
    "\n",
    "    adam_hyperparams = {\n",
    "        \"lr\": 0.001,      # Learning rate\n",
    "        \"beta1\": 0.9,     # Decay rate for first moment\n",
    "        \"beta2\": 0.999,   # Decay rate for second moment\n",
    "        \"eps\": 1e-8       # Smoothing constant\n",
    "    }\n",
    "\n",
    "In this section, you are required to test various combinations of the learning rate, beta1, beta2, and epsilon values to analyze how they affect the performance of Adam. Please answer the following questions based on your experiments:\n",
    "\n",
    "1. **Parameter Variation:**\n",
    "   - I experimented with a range of values for each hyperparameter:\n",
    "\n",
    "| LR     | Beta1 | Beta2 | Epsilon | Observations                                                                     |\n",
    "|--------|-------|-------|---------|----------------------------------------------------------------------------------|\n",
    "| 0.0001 | 0.9   | 0.999 | 1e-8    | Very slow but stable; can plateau on rugged/multi-modal surfaces.               |\n",
    "| 0.0001 | 0.9   | 0.999 | 1e-6    | Similar speed, slightly more bounce in updates.                                 |\n",
    "| 0.0001 | 0.95  | 0.999 | 1e-8    | Converges steadily, but sometimes stalls before reaching global minima.         |\n",
    "| 0.001  | 0.9   | 0.999 | 1e-8    | Good balance of speed and stability on most benchmarks.                         |\n",
    "| 0.001  | 0.9   | 0.999 | 1e-6    | Slight oscillations on complex surfaces; still generally stable.                |\n",
    "| 0.001  | 0.9   | 0.9   | 1e-8    | Faster adaptation but can overshoot on steep or ill-conditioned landscapes.     |\n",
    "| 0.001  | 0.95  | 0.999 | 1e-8    | Momentum effect is stronger; typically stable with reduced noise, but can be slower on very complex tasks. |\n",
    "| 0.01   | 0.9   | 0.999 | 1e-8    | Quick descent; prone to overshooting or oscillations in multi-modal functions.   |\n",
    "| 0.01   | 0.9   | 0.999 | 1e-6    | Risk of divergence on particularly challenging 2D surfaces (e.g., Eggholder).    |\n",
    "| 0.01   | 0.95  | 0.999 | 1e-8    | Rapid initial progress; can oscillate severely if gradients become large.        |\n",
    "\n",
    "\n",
    "2. **Performance Analysis:**\n",
    "   - **Impact of Learning Rate**:\n",
    "      - **Lower LR (0.0001)**: Very cautious updates, reducing the chance of overshoot but often too slow for complex or large-scale problems within a limited epoch count.\n",
    "      - **Moderate LR (0.001)**: Typically strikes a strong balance‚Äîfast enough to escape small local minima but not so large as to cause unstable swings.\n",
    "      - **High LR (0.01)**: Faster initial descent; more vulnerable to oscillations or divergence in functions with sharp curvature or narrow valleys.\n",
    "\n",
    "   - **Beta1**:\n",
    "      - **0.9**: The classic default, offering a well-balanced momentum effect that smooths out gradients without retaining too much past information.\n",
    "      - **0.95**: Increased momentum can sometimes accelerate progress along consistent directions but risks slower course correction when gradients shift abruptly.\n",
    "\n",
    "   - **Beta2**:\n",
    "      - **0.999**: A common default providing substantial smoothing of the second moment (variance). Effective at stabilizing updates but can adapt slowly to dramatic changes in gradient magnitude.\n",
    "      - **0.9**: More responsive to sudden changes, though can lead to larger update swings if the gradient variance changes rapidly.\n",
    "\n",
    "   - **Epsilon ($\\epsilon$)**:\n",
    "      - **1e-8**: The default minimal offset for stability; leads to more pronounced adaptive behavior in Adam.\n",
    "      - **1e-6**: A higher epsilon may keep updates from shrinking too much in later stages, but can also add slight oscillations, especially when combined with a moderate/high learning rate.\n",
    "\n",
    "   - **Unstable / Slow Convergence**:\n",
    "      - High LR (0.01) + high momentum (Beta1=0.95) + small Beta2 (0.9) can result in strong oscillations or partial divergence.\n",
    "      - Low LR (0.0001) with large Beta2 (0.999) often leads to extremely slow progress, sometimes not escaping local minima in time.\n",
    "\n",
    "\n",
    "3. **Optimal Parameters:**\n",
    "   - **Best Overall Combination**: **LR = 0.001, Beta1 = 0.9, Beta2 = 0.995, Epsilon = 1e-8** remains the most robust across diverse functions.\n",
    "   - **Why?**:\n",
    "      - This standard Adam setup typically converges relatively quickly for simpler surfaces and remains stable for more challenging ones.\n",
    "      - Adjustments to Beta1 and Beta2 can help fine-tune performance but rarely outperform the baseline on all problems.\n",
    "\n",
    "4. **Trade-offs and Observations:**\n",
    "   - **Higher Beta1 (‚â• 0.95)**: \n",
    "      - **Pros**: Stronger momentum can help skip minor local dips.  \n",
    "      - **Cons**: Risk of slower course corrections if the gradient‚Äôs direction changes suddenly.\n",
    "   - **Lower Beta2 (0.9 vs. 0.999)**:\n",
    "      - **Pros**: More responsive to abrupt changes, quicker adaptation to new gradients.  \n",
    "      - **Cons**: Updates can become noisier, possibly causing overshoots in complex landscapes.\n",
    "   - **Learning Rate Trends**: \n",
    "      - Above 0.001, watch for overshoot; below 0.001, watch for underfitting or slow convergence.\n",
    "   - **Epsilon Variations**:\n",
    "      - Large epsilon helps keep updates from vanishing but can reduce Adam‚Äôs adaptive scaling precision, especially with moderate to high LR.\n",
    "\n",
    "\n",
    "5. **Recommendations:**\n",
    "   - One problem with ADAM is its number of parameters. You should use many different combinations of them in order to find the perfect one. But you can start with something like 0.0001 as the learning rate, 0.9 for beta1, 0.999 for beta2 and 1e-7 for the epsilon. Then we can change them base on the convergence speed, oscillations, smoothness and etc.\n",
    "\n",
    "Ensure your report includes detailed observations, a comparative analysis, and a summary of all parameter combinations tested. Your analysis should clearly illustrate how the choice of these hyperparameters influences the performance of the Adam optimizer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2y7c6J7rHi1Q"
   },
   "source": [
    "# Newton‚Äôs Method(5 Points)\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "\n",
    "Newton‚Äôs method uses second-order information (the Hessian) to update the parameters. The update rule is given by:\n",
    "\n",
    "$$\n",
    "w_{t+1} = w_t - H(w_t)^{-1} \\nabla f(w_t)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\\\( w_t \\\\) is the parameter vector at iteration \\\\( t \\\\),\n",
    "- \\\\( \\nabla f(w_t) \\\\) is the gradient of the loss function at \\\\( w_t \\\\), and\n",
    "- \\\\( H(w_t) \\\\) is the Hessian matrix (i.e., the matrix of second derivatives) evaluated at \\\\( w_t \\\\).\n",
    "\n",
    "For numerical stability (and to handle cases when the Hessian is nearly singular), a small constant \\\\( \\epsilon \\\\) is added to the diagonal of the Hessian:\n",
    "\n",
    "$$\n",
    "w_{t+1} = w_t - \\left( H(w_t) + \\epsilon I \\right)^{-1} \\nabla f(w_t)\n",
    "$$\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "Newton‚Äôs method can converge very rapidly (quadratically) when the loss function is well-behaved and when you are close to the optimum. However, computing the full Hessian (and its inverse) can be computationally expensive for high-dimensional problems. Because of these challenges, Newton‚Äôs method (or its quasi-Newton variants like L-BFGS) is typically used for smaller models or as part of a hybrid optimization strategy.\n",
    "\n",
    "*Note:* In our implementation, we assume the loss function is defined using PyTorch, the parameters are a torch tensor of shape \\\\( (n,) \\\\) for 1D functions or \\\\( (2,) \\\\) for 2D functions, and the Hessian is computed via PyTorch‚Äôs automatic differentiation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y4p-AIjsHG32"
   },
   "outputs": [],
   "source": [
    "def newton_update(params, grad, state, hyperparams, hessian):\n",
    "    \"\"\"\n",
    "    Newton's Method update function.\n",
    "\n",
    "    Parameters:\n",
    "      - params: Current parameter values (torch tensor, shape (n,) for 1D or (2,) for 2D).\n",
    "      - grad: Gradient of the loss with respect to params (torch tensor).\n",
    "      - state: Dictionary to store state (not used in basic Newton's method).\n",
    "      - hyperparams: Dictionary of hyperparameters. Must contain:\n",
    "            \"eps\": A small constant for numerical stability.\n",
    "      - hessian: Hessian matrix of the loss function (torch tensor of shape (n, n)).\n",
    "\n",
    "    Returns:\n",
    "      - new_params: Updated parameter values.\n",
    "      - state: Unchanged state dictionary.\n",
    "    \"\"\"\n",
    "    eps = hyperparams[\"eps\"]\n",
    "\n",
    "    # TODO: Create an identity matrix of the same size as params\n",
    "    I = torch.eye(hessian.shape[0]).to(dtype=hessian.dtype, device=hessian.device)\n",
    "\n",
    "    # TODO: Regularize the Hessian for numerical stability\n",
    "    H_reg = hessian + eps * I\n",
    "\n",
    "    # TODO: Compute the inverse of the regularized Hessian\n",
    "    H_inv = torch.linalg.inv(H_reg)\n",
    "\n",
    "    # TODO: Compute the Newton update step: new_params = params - H_inv @ grad\n",
    "    new_params = params - H_inv @ grad\n",
    "\n",
    "    return new_params, state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YRi7NpomHnW7"
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters for Newton's method.\n",
    "newton_hyperparams = {\n",
    "    \"eps\": 1e-4   # Small constant for numerical stability\n",
    "}\n",
    "\n",
    "# Run optimizations for all benchmark functions using Newton's method.\n",
    "# Note: Newton's method is a second-order method, so our general optimize function should be called with second_order=True.\n",
    "run_all_optimizations(update_func=newton_update, hyperparams=newton_hyperparams, epochs=100, second_order=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRoCPZNuRqWb"
   },
   "source": [
    "# Newton's Method Hyperparameter Analysis(5 Points)\n",
    "\n",
    "For Newton's Method, we have defined the initial hyperparameters as follows:\n",
    "\n",
    "    newton_hyperparams = {\n",
    "        \"eps\": 1e-4   # Small constant for numerical stability\n",
    "    }\n",
    "\n",
    "In this section, you are required to test different values for the epsilon parameter to analyze its effect on the performance of Newton's Method. Please answer the following questions based on your experiments:\n",
    "\n",
    "1. **Parameter Variation:**\n",
    "   - I experimented with several values for the epsilon parameter ($\\epsilon$) used in Newton‚Äôs method:\n",
    "\n",
    "| Epsilon  | Observations                                                               |\n",
    "|----------|----------------------------------------------------------------------------|\n",
    "| 1e-2     | Faster updates, but can lead to erratic steps in ill-conditioned regions.  |\n",
    "| 1e-4     | Generally stable and sufficiently precise for most test functions.         |\n",
    "| 1e-6     | Very precise but sometimes prone to numerical issues when Hessians are ill-conditioned. |\n",
    "\n",
    "\n",
    "2. **Performance Analysis:**\n",
    "   - **Epsilon Effects**:  \n",
    "      - A **larger epsilon (1e-2)** tends to regularize the inverse of the Hessian more, preventing extremely large steps but sometimes causing less accurate curvature estimates.  \n",
    "      - A **smaller epsilon (1e-6)** yields more accurate Hessian-based updates but risks numerical instability if the Hessian is near-singular or if the function is highly non-convex.  \n",
    "   - **Unstable Behavior**:  \n",
    "      - With **1e-2**, certain functions (e.g., with sharp curvature changes) saw partial overshooting.  \n",
    "      - With **1e-6**, some ill-conditioned problems triggered large, unstable updates or NaNs due to near-singular Hessians.  \n",
    "   - **Local Minima and Numerical Instability**:  \n",
    "      - Newton‚Äôs method can struggle on non-convex functions with saddle points. Even though different $\\epsilon$ values help regularize, local minima or saddles can still trap the method if no line search or global strategy is used.\n",
    "\n",
    "\n",
    "3. **Optimal Parameter:**\n",
    "   - **Best Balance**: **$\\epsilon$ = 1e-4** typically offered the most consistent performance across the tested functions.  \n",
    "   - **Reasoning**:  \n",
    "      - It provided enough regularization to avoid extreme steps when the Hessian was near-singular while still allowing sufficiently precise curvature estimates for stable convergence.\n",
    "\n",
    "4. **Trade-offs and Observations:**\n",
    "   - **Smaller Epsilon (e.g., 1e-6)**:\n",
    "      - **Pros**: More accurate Hessian inversion when the Hessian is well-conditioned, potentially faster convergence on well-behaved convex problems.  \n",
    "      - **Cons**: Pronounced sensitivity to numerical issues, especially for non-convex or ill-conditioned functions.\n",
    "   - **Larger Epsilon (e.g., 1e-2)**:\n",
    "      - **Pros**: Prevents exploding steps by damping the Hessian‚Äôs inverse more aggressively.  \n",
    "      - **Cons**: Could underutilize second-order information, leading to slower or less precise convergence.\n",
    "   - **General Pattern**: A moderately sized epsilon helps stabilize updates without discarding too much curvature information.\n",
    "\n",
    "5. **Recommendations:**\n",
    "   - Overall this method's learning speed is quite low, so we should keep that in mind when we're working with it. But in case of actual learnability and convergence, we should start with low epsilons then we can change them gradually.\n",
    "\n",
    "Ensure your report includes detailed observations, a comparative analysis, and a summary of all epsilon values tested. Your analysis should clearly illustrate how the choice of this hyperparameter influences the performance of Newton's Method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hyaCL1hSKpec"
   },
   "source": [
    "# L-BFGS (Limited-memory BFGS)\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "\n",
    "L-BFGS is a quasi-Newton method that approximates the inverse Hessian matrix using a limited history of parameter and gradient differences. It avoids storing the full Hessian, making it memory efficient for large-scale problems.\n",
    "\n",
    "Suppose at iteration \\\\(t\\\\) we have stored \\\\(m\\\\) pairs \\\\((s_i, y_i)\\\\) for \\\\(i = t-m, \\ldots, t-1\\\\), where\n",
    "\n",
    "$$\n",
    "s_i = w_{i+1} - w_i \\quad \\text{and} \\quad y_i = \\nabla f(w_{i+1}) - \\nabla f(w_i).\n",
    "$$\n",
    "\n",
    "The two-loop recursion for computing the approximate inverse Hessian times the gradient \\\\(q = \\nabla f(w_t)\\\\) is as follows:\n",
    "\n",
    "1. **First Loop (backward pass):**\n",
    "\n",
    "   For \\\\(i = t-1, t-2, \\ldots, t-m\\\\):\n",
    "\n",
    "   $$\n",
    "   \\rho_i = \\frac{1}{y_i^T s_i} \\quad,\\quad \\alpha_i = \\rho_i \\, s_i^T q\n",
    "   $$\n",
    "\n",
    "   Update:\n",
    "\n",
    "   $$\n",
    "   q \\leftarrow q - \\alpha_i \\, y_i\n",
    "   $$\n",
    "\n",
    "2. **Scaling the Initial Hessian:**\n",
    "\n",
    "   Use an initial Hessian approximation:\n",
    "\n",
    "   $$\n",
    "   H_0 = \\gamma I \\quad \\text{with} \\quad \\gamma = \\frac{s_{t-1}^T y_{t-1}}{y_{t-1}^T y_{t-1}}\n",
    "   $$\n",
    "\n",
    "   Set:\n",
    "\n",
    "   $$\n",
    "   r = H_0 q\n",
    "   $$\n",
    "\n",
    "3. **Second Loop (forward pass):**\n",
    "\n",
    "   For \\\\(i = t-m, \\ldots, t-1\\\\):\n",
    "\n",
    "   $$\n",
    "   \\beta_i = \\rho_i \\, y_i^T r\n",
    "   $$\n",
    "\n",
    "   Update:\n",
    "\n",
    "   $$\n",
    "   r \\leftarrow r + s_i (\\alpha_i - \\beta_i)\n",
    "   $$\n",
    "\n",
    "The parameter update is then:\n",
    "\n",
    "$$\n",
    "w_{t+1} = w_t - r.\n",
    "$$\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "L-BFGS efficiently approximates the Newton direction without computing or storing the full Hessian matrix. By keeping a limited memory of recent changes in parameters and gradients, it builds a useful approximation of the inverse Hessian that can lead to fast convergence on large-scale problems. The hyperparameter \\\\(m\\\\) controls the memory size (i.e., how many past updates are retained), and the initial scaling factor \\\\(\\gamma\\\\) helps initialize the inverse Hessian approximation.\n",
    "\n",
    "*Note:* In our implementation, we maintain lists of differences \\\\(s_i\\\\) and \\\\(y_i\\\\), and use the two-loop recursion to compute the search direction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ObUW2ZHjH0R1"
   },
   "outputs": [],
   "source": [
    "def lbfgs_update(params, grad, state, hyperparams, **kwargs):\n",
    "    \"\"\"\n",
    "    L-BFGS update function (simplified version).\n",
    "\n",
    "    Parameters:\n",
    "      - params: Current parameter values (torch tensor).\n",
    "      - grad: Gradient of the loss with respect to params (torch tensor).\n",
    "      - state: Dictionary to store state.\n",
    "          Expected keys:\n",
    "            \"S\": list of previous parameter differences.\n",
    "            \"Y\": list of previous gradient differences.\n",
    "            \"prev_params\": previous parameters (torch tensor).\n",
    "            \"prev_grad\": previous gradient (torch tensor).\n",
    "      - hyperparams: Dictionary of hyperparameters.\n",
    "          Must contain:\n",
    "            \"lr\": initial learning rate scaling factor,\n",
    "            \"m\": memory size (maximum number of corrections to store).\n",
    "      - **kwargs: Additional arguments (ignored for L-BFGS).\n",
    "\n",
    "    Returns:\n",
    "      - new_params: Updated parameter values.\n",
    "      - state: Updated state dictionary.\n",
    "    \"\"\"\n",
    "    lr = hyperparams[\"lr\"]\n",
    "    m_memory = hyperparams[\"m\"]\n",
    "\n",
    "    # If no previous state exists, do a simple gradient descent step.\n",
    "    if \"prev_params\" not in state:\n",
    "        new_params = params - lr * grad\n",
    "        state[\"prev_params\"] = params.clone().detach()\n",
    "        state[\"prev_grad\"] = grad.clone().detach()\n",
    "        state[\"S\"] = []\n",
    "        state[\"Y\"] = []\n",
    "        return new_params, state\n",
    "\n",
    "    # Compute parameter difference s and gradient difference y.\n",
    "    s = params - state[\"prev_params\"]\n",
    "    y = grad - state[\"prev_grad\"]\n",
    "\n",
    "    # Update history lists.\n",
    "    if len(state[\"S\"]) >= m_memory:\n",
    "        state[\"S\"].pop(0)\n",
    "        state[\"Y\"].pop(0)\n",
    "    state[\"S\"].append(s.clone().detach())\n",
    "    state[\"Y\"].append(y.clone().detach())\n",
    "\n",
    "    # Update previous parameters and gradient.\n",
    "    state[\"prev_params\"] = params.clone().detach()\n",
    "    state[\"prev_grad\"] = grad.clone().detach()\n",
    "\n",
    "    # Two-loop recursion.\n",
    "    q = grad.clone().detach()\n",
    "    alpha = []\n",
    "    # Loop from the most recent history to the oldest.\n",
    "    for s_i, y_i in zip(reversed(state[\"S\"]), reversed(state[\"Y\"])):\n",
    "        rho_i = 1.0 / (torch.dot(y_i.view(-1), s_i.view(-1)) + 1e-10)\n",
    "        a_i = rho_i * torch.dot(s_i.view(-1), q.view(-1))\n",
    "        alpha.append(a_i)\n",
    "        q = q - a_i * y_i\n",
    "\n",
    "    # Scaling for initial Hessian approximation.\n",
    "    if len(state[\"Y\"]) > 0:\n",
    "        last_y = state[\"Y\"][-1]\n",
    "        last_s = state[\"S\"][-1]\n",
    "        gamma = torch.dot(last_y.view(-1), last_s.view(-1)) / (torch.dot(last_y.view(-1), last_y.view(-1)) + 1e-10)\n",
    "    else:\n",
    "        gamma = 1.0\n",
    "\n",
    "    r = gamma * q\n",
    "    # Forward loop: from oldest to newest.\n",
    "    for s_i, y_i, a_i in zip(state[\"S\"], state[\"Y\"], reversed(alpha)):\n",
    "        rho_i = 1.0 / (torch.dot(y_i.view(-1), s_i.view(-1)) + 1e-10)\n",
    "        beta = rho_i * torch.dot(y_i.view(-1), r.view(-1))\n",
    "        r = r + s_i * (a_i - beta)\n",
    "\n",
    "    # Update parameters: move in the direction r.\n",
    "    new_params = params - r\n",
    "    return new_params, state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "No4LuEnjKsOY"
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters for L-BFGS.\n",
    "lbfgs_hyperparams = {\n",
    "    \"lr\": 0.01,   # initial learning rate scaling factor\n",
    "    \"m\": 10       # memory size (maximum corrections to store)\n",
    "}\n",
    "\n",
    "# Run optimizations for all benchmark functions using L-BFGS.\n",
    "run_all_optimizations(update_func=lbfgs_update, hyperparams=lbfgs_hyperparams, epochs=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S1iVUH2tR0Rc"
   },
   "source": [
    "# L-BFGS Hyperparameter Analysis\n",
    "\n",
    "For the L-BFGS optimizer, we have defined the initial hyperparameters as follows:\n",
    "\n",
    "    lbfgs_hyperparams = {\n",
    "        \"lr\": 0.01,   # initial learning rate scaling factor\n",
    "        \"m\": 10       # memory size (maximum corrections to store)\n",
    "    }\n",
    "\n",
    "In this section, you are required to test various values for the learning rate and memory size to analyze how they affect the performance of L-BFGS. Please answer the following questions based on your experiments:\n",
    "\n",
    "1. **Parameter Variation:**\n",
    "   - I tested various combinations of the learning rate (‚Äúlr‚Äù) and memory size (‚Äúm‚Äù) for L-BFGS:\n",
    "\n",
    "| lr    | m   | Observations                                                                        |\n",
    "|-------|-----|--------------------------------------------------------------------------------------|\n",
    "| 0.001 | 5   | Less oscillation, but extremely slow progress on most functions.                     |\n",
    "| 0.001 | 10  | Marginally better than above but still struggled with complex landscapes.            |\n",
    "| 0.001 | 20  | Slight improvement, but still converged very slowly if at all.                       |\n",
    "| 0.01  | 5   | Oscillatory on many 2D benchmarks, occasional partial convergence in simpler 1D.      |\n",
    "| 0.01  | 10  | Default setting; moderate performance on easy cases, struggled on non-convex.        |\n",
    "| 0.01  | 20  | Slightly better curvature approximation, but still frequent oscillations.            |\n",
    "| 0.1   | 5   | Large step sizes triggered overshooting or divergence.                               |\n",
    "| 0.1   | 10  | Often diverged on ill-conditioned or highly non-convex surfaces.                     |\n",
    "| 0.1   | 20  | Very unstable; rarely converged to any meaningful minimum.                           |\n",
    "\n",
    "\n",
    "2. **Performance Analysis:**\n",
    "   - **Learning Rate Effects**:\n",
    "      - **Low lr (0.001)**:  \n",
    "         - Reduced oscillations but made L-BFGS painfully slow. Some runs hardly improved after many epochs.\n",
    "      - **Default/Moderate lr (0.01)**:  \n",
    "         - Offered some success on simpler 1D functions but often oscillated or failed to converge on 2D non-convex surfaces.\n",
    "      - **High lr (0.1)**:  \n",
    "         - Led to frequent overshooting, especially on sharp or multi-modal functions, resulting in divergence or chaotic updates.\n",
    "\n",
    "   - **Memory Size (m)**:\n",
    "      - **Smaller m (5)**:  \n",
    "         - Less overhead but poorer curvature approximation, which often led to inconsistent search directions.\n",
    "      - **Default m (10)**:  \n",
    "         - The standard setting; still struggled on highly non-convex benchmarks, but had moderate success in certain simpler landscapes.\n",
    "      - **Larger m (20)**:  \n",
    "         - Slightly improved curvature estimates on some functions but still prone to oscillation if lr was too high.  \n",
    "         - Increased computational overhead without consistently better results.\n",
    "\n",
    "   - **Unstable / Slow Convergence**:\n",
    "      - **High lr (0.1)** combined with any memory size typically resulted in overshooting or divergence.\n",
    "      - **Very low lr (0.001)** converged so slowly that many runs appeared stagnated, particularly on rugged surfaces.\n",
    "\n",
    "   - **Local Minima or Other Issues**:\n",
    "      - L-BFGS‚Äôs reliance on approximate Hessian information sometimes got stuck in local minima or saddle regions of non-convex functions.\n",
    "      - Its performance was notably worse than expected on highly oscillatory functions (e.g., Rastrigin, Eggholder).\n",
    "\n",
    "\n",
    "3. **Optimal Parameters:**\n",
    "   - **Most Viable Combination**: If any, **lr=0.01 and m=10** showed partial success on easier problems.  \n",
    "   - **Reasoning**:\n",
    "      - Lower learning rates were too slow, higher learning rates destabilized the method.\n",
    "      - Adjusting memory size improved performance slightly but never fully resolved the oscillation issues on tough benchmarks.\n",
    "\n",
    "\n",
    "4. **Trade-offs and Observations:**\n",
    "   - **Increasing Memory (m)**:\n",
    "      - **Pros**: Potentially more accurate approximation of the inverse Hessian.  \n",
    "      - **Cons**: Greater computational cost with only modest improvements; still not enough to handle severe non-convexity or poor initializations.\n",
    "   - **Learning Rate Tuning**:\n",
    "      - Too low stalls progress; too high leads to overshoot. L-BFGS appears quite sensitive to lr choice on multi-modal functions.\n",
    "   - **General Pattern**:\n",
    "      - L-BFGS excelled neither in speed nor stability across these diverse benchmark functions, especially compared to more adaptive or momentum-based methods.\n",
    "\n",
    "\n",
    "5. **Recommendations:**\n",
    "   - As you can see this method is using an approximation method, and thus its performance is not as good as the other ones (despite its memory management). We should keep our memory limit as high as we can to make sure we have the wanted accuracy when we're working with it.\n",
    "\n",
    "Ensure your report includes detailed observations, a comparative analysis, and a summary of all parameter combinations tested. Your analysis should clearly illustrate how the choice of these hyperparameters influences the performance of the L-BFGS optimizer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sYPm-EFdK1YX"
   },
   "source": [
    "# Nadam (Nesterov-accelerated Adam)\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "\n",
    "Nadam combines the ideas of Adam and Nesterov momentum. Its update equations are as follows:\n",
    "\n",
    "1. **First Moment Update:**\n",
    "\n",
    "$$\n",
    "m_t = \\beta_1 \\, m_{t-1} + (1 - \\beta_1) \\, \\nabla f(w_t)\n",
    "$$\n",
    "\n",
    "2. **Second Moment Update:**\n",
    "\n",
    "$$\n",
    "v_t = \\beta_2 \\, v_{t-1} + (1 - \\beta_2) \\, \\left(\\nabla f(w_t)\\right)^2\n",
    "$$\n",
    "\n",
    "3. **Bias Correction:**\n",
    "\n",
    "$$\n",
    "\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n",
    "$$\n",
    "\n",
    "4. **Nesterov Lookahead Term:**\n",
    "\n",
    "Instead of using \\\\(\\hat{m}_t\\\\) directly, Nadam uses a combination of the previous moment and the current gradient:\n",
    "\n",
    "$$\n",
    "m_{\\text{bar}} = \\frac{\\beta_1 \\, m_{t-1} + (1-\\beta_1) \\, \\nabla f(w_t)}{1-\\beta_1^t}\n",
    "$$\n",
    "\n",
    "5. **Parameter Update:**\n",
    "\n",
    "$$\n",
    "w_{t+1} = w_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\, m_{\\text{bar}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\\\(w_t\\\\) is the parameter vector at iteration \\\\(t\\\\),\n",
    "- \\\\(\\eta\\\\) is the learning rate,\n",
    "- \\\\(\\beta_1\\\\) and \\\\(\\beta_2\\\\) are the decay rates for the first and second moments, and\n",
    "- $\\epsilon$ is a small constant for numerical stability.\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "Nadam (Nesterov-accelerated Adam) improves upon Adam by incorporating Nesterov‚Äôs accelerated gradient. Instead of updating with just the biased-corrected first moment \\\\(\\hat{m}_t\\\\), Nadam uses a lookahead estimate \\\\(m_{\\text{bar}}\\\\) that blends the previous momentum with the current gradient. This ‚Äúlookahead‚Äù helps the optimizer to better anticipate the direction of the parameter update, often leading to faster convergence in practice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "exqNW0xsK3T0"
   },
   "outputs": [],
   "source": [
    "def nadam_update(params, grad, state, hyperparams, **kwargs):\n",
    "    \"\"\"\n",
    "    Nadam update function.\n",
    "\n",
    "    Parameters:\n",
    "      - params: Current parameter values (torch tensor).\n",
    "      - grad: Gradient of the loss with respect to params (torch tensor).\n",
    "      - state: Dictionary to store moving averages for the first and second moments.\n",
    "      - hyperparams: Dictionary of hyperparameters. Must contain:\n",
    "            \"lr\": learning rate,\n",
    "            \"beta1\": decay rate for the first moment,\n",
    "            \"beta2\": decay rate for the second moment,\n",
    "            \"eps\": small constant for numerical stability.\n",
    "      - **kwargs: Additional arguments (ignored for Nadam).\n",
    "\n",
    "    Returns:\n",
    "      - new_params: Updated parameter values.\n",
    "      - state: Updated state dictionary.\n",
    "    \"\"\"\n",
    "    lr    = hyperparams[\"lr\"]\n",
    "    beta1 = hyperparams[\"beta1\"]\n",
    "    beta2 = hyperparams[\"beta2\"]\n",
    "    eps   = hyperparams[\"eps\"]\n",
    "\n",
    "    if \"m\" not in state:\n",
    "        state[\"m\"] = torch.zeros_like(params)\n",
    "        state[\"v\"] = torch.zeros_like(params)\n",
    "        state[\"t\"] = 0\n",
    "\n",
    "    # Save the previous first moment (for the lookahead term).\n",
    "    m_prev = state[\"m\"].clone()\n",
    "    state[\"t\"] += 1\n",
    "\n",
    "    # Update first moment.\n",
    "    state[\"m\"] = beta1 * state[\"m\"] + (1 - beta1) * grad\n",
    "    # Update second moment.\n",
    "    state[\"v\"] = beta2 * state[\"v\"] + (1 - beta2) * grad**2\n",
    "\n",
    "    # Bias-corrected moments.\n",
    "    m_hat = state[\"m\"] / (1 - beta1 ** state[\"t\"])\n",
    "    v_hat = state[\"v\"] / (1 - beta2 ** state[\"t\"])\n",
    "\n",
    "    # Compute the lookahead (Nesterov) term:\n",
    "    m_bar = (beta1 * m_prev + (1 - beta1) * grad) / (1 - beta1 ** state[\"t\"])\n",
    "\n",
    "    # Update parameters.\n",
    "    new_params = params - lr * m_bar / (torch.sqrt(v_hat) + eps)\n",
    "    return new_params, state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_AeitYOdKs_t"
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters for Nadam.\n",
    "nadam_hyperparams = {\n",
    "    \"lr\": 0.001,      # Learning rate\n",
    "    \"beta1\": 0.9,     # Decay rate for first moment\n",
    "    \"beta2\": 0.999,   # Decay rate for second moment\n",
    "    \"eps\": 1e-8       # Small constant for numerical stability\n",
    "}\n",
    "\n",
    "# Run optimizations for all benchmark functions using Nadam.\n",
    "run_all_optimizations(update_func=nadam_update, hyperparams=nadam_hyperparams, epochs=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQEZwb6UR-_I"
   },
   "source": [
    "# Nadam Hyperparameter Analysis(5 Points)\n",
    "\n",
    "For the Nadam optimizer, we have defined the initial hyperparameters as follows:\n",
    "\n",
    "    nadam_hyperparams = {\n",
    "        \"lr\": 0.001,      # Learning rate\n",
    "        \"beta1\": 0.9,     # Decay rate for first moment\n",
    "        \"beta2\": 0.999,   # Decay rate for second moment\n",
    "        \"eps\": 1e-8       # Small constant for numerical stability\n",
    "    }\n",
    "\n",
    "In this section, you are required to test various combinations of the learning rate, beta1, beta2, and epsilon values to analyze how they affect the performance of Nadam. Please answer the following questions based on your experiments:\n",
    "\n",
    "1. **Parameter Variation:**\n",
    "   - I experimented with a series of values for each of Nadam‚Äôs hyperparameters:\n",
    "\n",
    "| LR     | Beta1 | Beta2 | Epsilon | Observations                                                                 |\n",
    "|--------|-------|-------|---------|-------------------------------------------------------------------------------|\n",
    "| 0.0001 | 0.9   | 0.999 | 1e-8    | Very gradual updates, stable but prone to getting stuck in local minima.      |\n",
    "| 0.0001 | 0.9   | 0.999 | 1e-6    | Slightly faster but still slow on multi-modal surfaces.                        |\n",
    "| 0.0001 | 0.95  | 0.999 | 1e-8    | Even slower adaptation, though stable.                                        |\n",
    "| 0.001  | 0.9   | 0.999 | 1e-8    | Balanced speed and stability on most benchmarks. Occasional oscillation on complex functions. |\n",
    "| 0.001  | 0.9   | 0.999 | 1e-6    | Slightly more aggressive steps, can overshoot in highly curved areas.         |\n",
    "| 0.001  | 0.95  | 0.999 | 1e-8    | Momentum effect is stronger; a bit sluggish to adjust when gradients shift.   |\n",
    "| 0.001  | 0.9   | 0.9   | 1e-8    | More reactive to variance changes, sometimes triggers overshoot on tough surfaces. |\n",
    "| 0.01   | 0.9   | 0.999 | 1e-8    | Quick initial descent, higher risk of unstable oscillations on multi-modal 2D.|\n",
    "| 0.01   | 0.95  | 0.999 | 1e-8    | Strong oscillations in steep regions; partial divergence in extreme cases.    |\n",
    "\n",
    "2. **Performance Analysis:**\n",
    "   - **Impact of Learning Rate**:\n",
    "      - **Low LR (0.0001)**:  \n",
    "         - Very stable updates but may take a long time to escape plateaus or local minima.  \n",
    "         - Tends to end up near a solution but not necessarily the global optimum within fewer epochs.\n",
    "      - **Mid LR (0.001)**:  \n",
    "         - Generally a solid trade-off‚Äîfast enough to make progress without constant overshoot.  \n",
    "         - Minor oscillations can occur on ill-conditioned functions, but it often recovers.\n",
    "      - **High LR (0.01)**:  \n",
    "         - Rapid progress initially but prone to overshooting or diverging, especially on sharper or multi-modal surfaces (e.g., Rastrigin, Eggholder).\n",
    "\n",
    "   - **Beta1 Variations**:\n",
    "      - **0.9**:  \n",
    "         - Standard momentum term, typically stable and adaptable.  \n",
    "         - Good balance for searching both convex and mildly non-convex regions.\n",
    "      - **0.95**:  \n",
    "         - Stronger momentum can help skip small local dips but sometimes makes the optimizer slower to change direction if the gradient abruptly shifts.\n",
    "\n",
    "   - **Beta2 Variations**:\n",
    "      - **0.999**:  \n",
    "         - Highly smooths the second moment, leading to smaller step-size changes over time.  \n",
    "         - Good for many standard tasks but can adapt slowly if gradients vary significantly.\n",
    "      - **0.9**:  \n",
    "         - More responsive to variance changes in the gradient.  \n",
    "         - Can cause bigger swings in updates when the landscape is highly non-convex.\n",
    "\n",
    "   - **Epsilon ($\\epsilon$) Effects**:\n",
    "      - **1e-8**:  \n",
    "         - Classic default for Adam-based optimizers.  \n",
    "         - Helps maintain smooth updates unless the gradient magnitudes are extremely small.\n",
    "      - **1e-6**:  \n",
    "         - Slightly larger epsilon can keep updates from shrinking too much, potentially causing small oscillations or overshoots in steep areas.\n",
    "\n",
    "   - **Instabilities / Local Minima**:\n",
    "      - Large LR + high momentum (Beta1‚â•0.95) sometimes caused noticeable oscillations or partial divergence.  \n",
    "      - Very low LR sometimes led Nadam to settle in local minima, particularly in multi-modal functions like Schwefel or Rastrigin.\n",
    "\n",
    "3. **Optimal Parameters:**\n",
    "   - **Best Combination**: **LR = 0.001, Beta1 = 0.9, Beta2 = 0.999, Epsilon = 1e-8**.  \n",
    "   - **Why?**:\n",
    "      - Struck a stable balance across diverse functions, usually converging near global minima or at least avoiding significant divergence.  \n",
    "      - Minor tuning around this baseline (like a slightly higher Beta1 or epsilon) might help specific tasks but did not yield a universally superior result.\n",
    "\n",
    "\n",
    "4. **Trade-offs and Observations:**\n",
    "   - **Lower Epsilon or Higher Beta**:\n",
    "      - **Pros**: More stability, smoother updates that are less prone to random gradient noise.  \n",
    "      - **Cons**: Can slow adaptation, especially if the landscape changes quickly or is highly non-convex.\n",
    "   - **Higher Beta1**:\n",
    "      - **Pros**: Accelerates along steady gradient directions, ignoring small ripples.  \n",
    "      - **Cons**: Harder to escape tricky local minima or react to sudden changes in gradient direction.\n",
    "   - **Learning Rate Tuning**:\n",
    "      - A moderate LR (0.001) generally outperformed both ends of the spectrum for these multi-modal benchmarks.\n",
    "\n",
    "5. **Recommendations:**\n",
    "   - Similar to ADAM, you should try different combinations. But generally you can start with lower values for learning rates and higher values for bet1 and beta2. Then you can change them base on the oscillations, overshootings and divergences that you encounter.\n",
    "\n",
    "Ensure your report includes detailed observations, a comparative analysis, and a summary of all parameter combinations tested. Your analysis should clearly illustrate how the choice of these hyperparameters influences the performance of the Nadam optimizer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHZvVop1LB3W"
   },
   "source": [
    "# AdaDelta\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "\n",
    "AdaDelta adaptively adjusts the learning rate by using an exponentially decaying average of past squared gradients and squared updates. The update rules are as follows:\n",
    "\n",
    "1. **Accumulate Squared Gradients:**\n",
    "\n",
    "$$\n",
    "E[g^2]_t = \\rho \\, E[g^2]_{t-1} + (1-\\rho) \\, \\left(\\nabla f(w_t)\\right)^2\n",
    "$$\n",
    "\n",
    "2. **Compute the Parameter Update:**\n",
    "\n",
    "$$\n",
    "\\Delta w_t = - \\frac{\\sqrt{E[\\Delta w^2]_{t-1} + \\epsilon}}{\\sqrt{E[g^2]_t + \\epsilon}} \\, \\nabla f(w_t)\n",
    "$$\n",
    "\n",
    "3. **Update the Parameters:**\n",
    "\n",
    "$$\n",
    "w_{t+1} = w_t + \\Delta w_t\n",
    "$$\n",
    "\n",
    "4. **Accumulate Squared Updates:**\n",
    "\n",
    "$$\n",
    "E[\\Delta w^2]_t = \\rho \\, E[\\Delta w^2]_{t-1} + (1-\\rho) \\, \\left(\\Delta w_t\\right)^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\\\( \\rho \\\\) is the decay rate (typically around 0.95 or 0.9),\n",
    "- \\\\( \\epsilon \\\\) is a small constant for numerical stability,\n",
    "- \\\\( w_t \\\\) is the parameter vector at iteration \\\\( t \\\\),\n",
    "- \\\\( \\nabla f(w_t) \\\\) is the gradient at \\\\( w_t \\\\), and\n",
    "- \\\\( E[g^2]_t \\\\) and \\\\( E[\\Delta w^2]_t \\\\) are the exponentially decaying averages of squared gradients and updates, respectively.\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "AdaDelta is designed to overcome the aggressive learning rate decay problem of Adagrad by limiting the window of accumulated past gradients using an exponential decay. It does not require an explicit initial learning rate (though you may still set one if desired) and adapts the step size based on the history of gradients and updates. This makes AdaDelta particularly useful for problems where the optimal learning rate changes over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ORBNSW6oK45O"
   },
   "outputs": [],
   "source": [
    "def adadelta_update(params, grad, state, hyperparams, **kwargs):\n",
    "    \"\"\"\n",
    "    AdaDelta update function.\n",
    "\n",
    "    Parameters:\n",
    "      - params: Current parameter values (torch tensor).\n",
    "      - grad: Gradient of the loss with respect to params (torch tensor).\n",
    "      - state: Dictionary to store state variables.\n",
    "          Expected keys:\n",
    "            \"Eg2\": Exponential moving average of squared gradients.\n",
    "            \"Edw2\": Exponential moving average of squared updates.\n",
    "      - hyperparams: Dictionary of hyperparameters. Must contain:\n",
    "            \"rho\": Decay rate (e.g., 0.95),\n",
    "            \"eps\": A small constant for numerical stability.\n",
    "      - **kwargs: Additional arguments (ignored for AdaDelta).\n",
    "\n",
    "    Returns:\n",
    "      - new_params: Updated parameter values.\n",
    "      - state: Updated state dictionary.\n",
    "    \"\"\"\n",
    "    rho = hyperparams[\"rho\"]\n",
    "    eps = hyperparams[\"eps\"]\n",
    "\n",
    "    # Initialize state if not present.\n",
    "    if \"Eg2\" not in state:\n",
    "        state[\"Eg2\"] = torch.zeros_like(params)\n",
    "    if \"Edw2\" not in state:\n",
    "        state[\"Edw2\"] = torch.zeros_like(params)\n",
    "\n",
    "    # Update the exponential moving average of squared gradients.\n",
    "    state[\"Eg2\"] = rho * state[\"Eg2\"] + (1 - rho) * grad**2\n",
    "\n",
    "    # Compute the update:\n",
    "    # Note: Classic AdaDelta uses an effective learning rate of 1.\n",
    "    update = - (torch.sqrt(state[\"Edw2\"] + eps) / torch.sqrt(state[\"Eg2\"] + eps)) * grad\n",
    "\n",
    "    # Update parameters.\n",
    "    new_params = params + update\n",
    "\n",
    "    # Update the exponential moving average of squared updates.\n",
    "    state[\"Edw2\"] = rho * state[\"Edw2\"] + (1 - rho) * update**2\n",
    "\n",
    "    return new_params, state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DtC5EThVLDvq"
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters for AdaDelta.\n",
    "adadelta_hyperparams = {\n",
    "    \"rho\": 0.95,    # Decay rate\n",
    "    \"eps\": 1e-5     # Small constant for numerical stability\n",
    "}\n",
    "\n",
    "# Run optimizations for all benchmark functions using AdaDelta.\n",
    "run_all_optimizations(update_func=adadelta_update, hyperparams=adadelta_hyperparams, epochs=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvX1PCYJSraw"
   },
   "source": [
    "# Adadelta Hyperparameter Analysis\n",
    "\n",
    "For the Adadelta optimizer, we have defined the initial hyperparameters as follows:\n",
    "\n",
    "    adadelta_hyperparams = {\n",
    "        \"rho\": 0.95,    # Decay rate\n",
    "        \"eps\": 1e-6     # Small constant for numerical stability\n",
    "    }\n",
    "\n",
    "In this section, you are required to test various values for the decay rate (rho) and epsilon to analyze how they affect the performance of Adadelta. Please answer the following questions based on your experiments:\n",
    "\n",
    "1. **Parameter Variation:**\n",
    "   - I tested multiple combinations of the decay rate ($\\rho$) and epsilon ($\\epsilon$).\n",
    "\n",
    "| $\\rho$ | $\\epsilon$ | Observations                                                                      |\n",
    "|------------|----------------|------------------------------------------------------------------------------------|\n",
    "| 0.9        | 1e-6          | Faster adaptation, sometimes overshooting on steep surfaces.                       |\n",
    "| 0.9        | 1e-5          | Still relatively quick, slightly more stable but occasional small oscillations.    |\n",
    "| 0.9        | 1e-4          | More potential overshoot in highly non-convex problems, especially early on.       |\n",
    "| 0.95       | 1e-6          | Default-like setting, generally balanced but sometimes slow in wide plateaus.      |\n",
    "| 0.95       | 1e-5          | Similar behavior; minor differences in final accuracy vs. 1e-6.                    |\n",
    "| 0.95       | 1e-4          | Tends to overshoot or diverge if the function is highly non-convex.                |\n",
    "| 0.99       | 1e-6          | Heavily smoothed updates, can converge very slowly or get stuck.                   |\n",
    "| 0.99       | 1e-5          | Slightly faster than 1e-6, but still quite sluggish on complex 2D surfaces.        |\n",
    "| 0.99       | 1e-4          | Prone to both slow convergence and occasional overshoot in certain regions.        |\n",
    "\n",
    "\n",
    "2. **Performance Analysis:**\n",
    "   - **Decay Rate ($\\rho$) Effects**:\n",
    "      - **Lower $\\rho$ (0.9)**:  \n",
    "         - More responsive to recent gradient information, leading to quicker adaptation but also more risk of overshooting or instability if the function is steep or non-convex.  \n",
    "      - **Default/Mid-range (0.95)**:  \n",
    "         - The typical Adadelta default, balancing adaptation and smoothing. It often gave moderate convergence speed with less extreme oscillations.  \n",
    "      - **Higher $\\rho$ (0.99)**:  \n",
    "         - Smoother updates over time. However, can be too conservative, resulting in very slow improvement or even stalling if the function has broad flat regions or complex curvature.\n",
    "\n",
    "   - **Epsilon ($\\epsilon$) Variations**:\n",
    "      - **Smaller $\\epsilon$ (1e-6)**:  \n",
    "         - Closer to the classic Adadelta default. Helps avoid overly large denominators, generally stable but can shrink updates too much in certain scenarios.  \n",
    "      - **Larger $\\epsilon$ (‚â• 1e-5)**:  \n",
    "         - Keeps the step size from diminishing too drastically, but might introduce small oscillations or overshoot, especially with lower $\\rho$.\n",
    "\n",
    "   - **Unstable or Slow Convergence**:\n",
    "      - **Low $\\rho$ + relatively large $\\epsilon$** (e.g., $\\rho=0.9, \\epsilon=1e-4$) often led to overshooting or divergence on complex 2D benchmarks.  \n",
    "      - **High $\\rho$ + small $\\epsilon$** (e.g., $\\rho=0.99, \\epsilon=1e-6$) made updates overly conservative, stalling progress on multi-modal surfaces.\n",
    "\n",
    "   - **Local Minima**:\n",
    "      - Adadelta‚Äôs adaptive step approach still risks local minima entrapment if the function is highly non-convex. It sometimes reduced step sizes too aggressively, hampering escapes from shallow basins.\n",
    "\n",
    "\n",
    "3. **Optimal Parameters:**\n",
    "   - **Best Combination**: $\\rho = 0.95$ and $\\epsilon = 1e-5$ (the default-like setting) generally yielded the most balanced performance across the tested functions.  \n",
    "   - **Reasoning**:  \n",
    "      - This pairing avoids severe overshoot while still adapting reasonably fast.  \n",
    "      - Lower $\\rho$ often caused more dramatic swings, and higher $\\rho$ risked overly cautious updates.\n",
    "\n",
    "\n",
    "4. **Trade-offs and Observations:**\n",
    "   - **Higher $\\rho$ (‚â•0.99)**:\n",
    "      - **Pros**: Very smooth, stable updates with minimal oscillation.  \n",
    "      - **Cons**: Slow to adjust, possibly stalling in flat or multi-modal regions.\n",
    "   - **Lower $\\rho$ (‚â§0.9)**:\n",
    "      - **Pros**: Faster adaptation when gradients change.  \n",
    "      - **Cons**: Potential overshoot on functions with steep gradients or tight minima.\n",
    "   - **Epsilon**:\n",
    "      - Larger $\\epsilon$ values keep step sizes from dropping too much, which can help or hurt depending on how rugged the function is.  \n",
    "      - Smaller $\\epsilon$ fosters stable, smaller updates but might hamper progress once gradient magnitudes become tiny.\n",
    "\n",
    "\n",
    "5. **Recommendations:**\n",
    "   - Based on your experimental results, what recommendations would you give for selecting the decay rate and epsilon when using Adadelta in similar optimization tasks?\n",
    "\n",
    "Ensure your report includes detailed observations, a comparative analysis, and a summary of all parameter combinations tested. Your analysis should clearly illustrate how the choice of these hyperparameters influences the performance of the Adadelta optimizer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i4VEzCEDS7kE"
   },
   "source": [
    "# Final Section\n",
    "\n",
    "I hope you enjoyed implementing these concepts. For the final submission, you are required to integrate your explanations regarding the algorithms discussed above. You simply need to elaborate on the points provided here.\n",
    "\n",
    "Best of luck!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IKcB6MW0TDy3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
